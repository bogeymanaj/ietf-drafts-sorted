%!PS-Adobe-3.0
%%BoundingBox: 18 36 577 806
%%Title: Enscript Output
%%Creator: GNU Enscript 1.6.5.2
%%CreationDate: Mon Mar  7 23:34:33 2011
%%Orientation: Portrait
%%Pages: (atend)
%%DocumentMedia: A4 595 842 0 () ()
%%DocumentNeededResources: (atend)
%%EndComments
%%BeginProlog
%%BeginResource: procset Enscript-Prolog 1.6.5 2
%
% Procedures.
%

/_S {	% save current state
  /_s save def
} def
/_R {	% restore from saved state
  _s restore
} def

/S {	% showpage protecting gstate
  gsave
  showpage
  grestore
} bind def

/MF {	% fontname newfontname -> -	make a new encoded font
  /newfontname exch def
  /fontname exch def

  /fontdict fontname findfont def
  /newfont fontdict maxlength dict def

  fontdict {
    exch
    dup /FID eq {
      % skip FID pair
      pop pop
    } {
      % copy to the new font dictionary
      exch newfont 3 1 roll put
    } ifelse
  } forall

  newfont /FontName newfontname put

  % insert only valid encoding vectors
  encoding_vector length 256 eq {
    newfont /Encoding encoding_vector put
  } if

  newfontname newfont definefont pop
} def

/MF_PS { % fontname newfontname -> -	make a new font preserving its enc
  /newfontname exch def
  /fontname exch def

  /fontdict fontname findfont def
  /newfont fontdict maxlength dict def

  fontdict {
    exch
    dup /FID eq {
      % skip FID pair
      pop pop
    } {
      % copy to the new font dictionary
      exch newfont 3 1 roll put
    } ifelse
  } forall

  newfont /FontName newfontname put

  newfontname newfont definefont pop
} def

/SF { % fontname width height -> -	set a new font
  /height exch def
  /width exch def

  findfont
  [width 0 0 height 0 0] makefont setfont
} def

/SUF { % fontname width height -> -	set a new user font
  /height exch def
  /width exch def

  /F-gs-user-font MF
  /F-gs-user-font width height SF
} def

/SUF_PS { % fontname width height -> -	set a new user font preserving its enc
  /height exch def
  /width exch def

  /F-gs-user-font MF_PS
  /F-gs-user-font width height SF
} def

/M {moveto} bind def
/s {show} bind def

/Box {	% x y w h -> -			define box path
  /d_h exch def /d_w exch def /d_y exch def /d_x exch def
  d_x d_y  moveto
  d_w 0 rlineto
  0 d_h rlineto
  d_w neg 0 rlineto
  closepath
} def

/bgs {	% x y height blskip gray str -> -	show string with bg color
  /str exch def
  /gray exch def
  /blskip exch def
  /height exch def
  /y exch def
  /x exch def

  gsave
    x y blskip sub str stringwidth pop height Box
    gray setgray
    fill
  grestore
  x y M str s
} def

/bgcs { % x y height blskip red green blue str -> -  show string with bg color
  /str exch def
  /blue exch def
  /green exch def
  /red exch def
  /blskip exch def
  /height exch def
  /y exch def
  /x exch def

  gsave
    x y blskip sub str stringwidth pop height Box
    red green blue setrgbcolor
    fill
  grestore
  x y M str s
} def

% Highlight bars.
/highlight_bars {	% nlines lineheight output_y_margin gray -> -
  gsave
    setgray
    /ymarg exch def
    /lineheight exch def
    /nlines exch def

    % This 2 is just a magic number to sync highlight lines to text.
    0 d_header_y ymarg sub 2 sub translate

    /cw d_output_w cols div def
    /nrows d_output_h ymarg 2 mul sub lineheight div cvi def

    % for each column
    0 1 cols 1 sub {
      cw mul /xp exch def

      % for each rows
      0 1 nrows 1 sub {
        /rn exch def
        rn lineheight mul neg /yp exch def
        rn nlines idiv 2 mod 0 eq {
	  % Draw highlight bar.  4 is just a magic indentation.
	  xp 4 add yp cw 8 sub lineheight neg Box fill
	} if
      } for
    } for

  grestore
} def

% Line highlight bar.
/line_highlight {	% x y width height gray -> -
  gsave
    /gray exch def
    Box gray setgray fill
  grestore
} def

% Column separator lines.
/column_lines {
  gsave
    .1 setlinewidth
    0 d_footer_h translate
    /cw d_output_w cols div def
    1 1 cols 1 sub {
      cw mul 0 moveto
      0 d_output_h rlineto stroke
    } for
  grestore
} def

% Column borders.
/column_borders {
  gsave
    .1 setlinewidth
    0 d_footer_h moveto
    0 d_output_h rlineto
    d_output_w 0 rlineto
    0 d_output_h neg rlineto
    closepath stroke
  grestore
} def

% Do the actual underlay drawing
/draw_underlay {
  ul_style 0 eq {
    ul_str true charpath stroke
  } {
    ul_str show
  } ifelse
} def

% Underlay
/underlay {	% - -> -
  gsave
    0 d_page_h translate
    d_page_h neg d_page_w atan rotate

    ul_gray setgray
    ul_font setfont
    /dw d_page_h dup mul d_page_w dup mul add sqrt def
    ul_str stringwidth pop dw exch sub 2 div ul_h_ptsize -2 div moveto
    draw_underlay
  grestore
} def

/user_underlay {	% - -> -
  gsave
    ul_x ul_y translate
    ul_angle rotate
    ul_gray setgray
    ul_font setfont
    0 0 ul_h_ptsize 2 div sub moveto
    draw_underlay
  grestore
} def

% Page prefeed
/page_prefeed {		% bool -> -
  statusdict /prefeed known {
    statusdict exch /prefeed exch put
  } {
    pop
  } ifelse
} def

% Wrapped line markers
/wrapped_line_mark {	% x y charwith charheight type -> -
  /type exch def
  /h exch def
  /w exch def
  /y exch def
  /x exch def

  type 2 eq {
    % Black boxes (like TeX does)
    gsave
      0 setlinewidth
      x w 4 div add y M
      0 h rlineto w 2 div 0 rlineto 0 h neg rlineto
      closepath fill
    grestore
  } {
    type 3 eq {
      % Small arrows
      gsave
        .2 setlinewidth
        x w 2 div add y h 2 div add M
        w 4 div 0 rlineto
        x w 4 div add y lineto stroke

        x w 4 div add w 8 div add y h 4 div add M
        x w 4 div add y lineto
	w 4 div h 8 div rlineto stroke
      grestore
    } {
      % do nothing
    } ifelse
  } ifelse
} def

% EPSF import.

/BeginEPSF {
  /b4_Inc_state save def    		% Save state for cleanup
  /dict_count countdictstack def	% Count objects on dict stack
  /op_count count 1 sub def		% Count objects on operand stack
  userdict begin
  /showpage { } def
  0 setgray 0 setlinecap
  1 setlinewidth 0 setlinejoin
  10 setmiterlimit [ ] 0 setdash newpath
  /languagelevel where {
    pop languagelevel
    1 ne {
      false setstrokeadjust false setoverprint
    } if
  } if
} bind def

/EndEPSF {
  count op_count sub { pos } repeat	% Clean up stacks
  countdictstack dict_count sub { end } repeat
  b4_Inc_state restore
} bind def

% Check PostScript language level.
/languagelevel where {
  pop /gs_languagelevel languagelevel def
} {
  /gs_languagelevel 1 def
} ifelse
%%EndResource
%%BeginResource: procset Enscript-Encoding-88591 1.6.5 2
/encoding_vector [
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/space        	/exclam       	/quotedbl     	/numbersign   	
/dollar       	/percent      	/ampersand    	/quoteright   	
/parenleft    	/parenright   	/asterisk     	/plus         	
/comma        	/hyphen       	/period       	/slash        	
/zero         	/one          	/two          	/three        	
/four         	/five         	/six          	/seven        	
/eight        	/nine         	/colon        	/semicolon    	
/less         	/equal        	/greater      	/question     	
/at           	/A            	/B            	/C            	
/D            	/E            	/F            	/G            	
/H            	/I            	/J            	/K            	
/L            	/M            	/N            	/O            	
/P            	/Q            	/R            	/S            	
/T            	/U            	/V            	/W            	
/X            	/Y            	/Z            	/bracketleft  	
/backslash    	/bracketright 	/asciicircum  	/underscore   	
/quoteleft    	/a            	/b            	/c            	
/d            	/e            	/f            	/g            	
/h            	/i            	/j            	/k            	
/l            	/m            	/n            	/o            	
/p            	/q            	/r            	/s            	
/t            	/u            	/v            	/w            	
/x            	/y            	/z            	/braceleft    	
/bar          	/braceright   	/tilde        	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/.notdef      	/.notdef      	/.notdef      	/.notdef      	
/space        	/exclamdown   	/cent         	/sterling     	
/currency     	/yen          	/brokenbar    	/section      	
/dieresis     	/copyright    	/ordfeminine  	/guillemotleft	
/logicalnot   	/hyphen       	/registered   	/macron       	
/degree       	/plusminus    	/twosuperior  	/threesuperior	
/acute        	/mu           	/paragraph    	/bullet       	
/cedilla      	/onesuperior  	/ordmasculine 	/guillemotright	
/onequarter   	/onehalf      	/threequarters	/questiondown 	
/Agrave       	/Aacute       	/Acircumflex  	/Atilde       	
/Adieresis    	/Aring        	/AE           	/Ccedilla     	
/Egrave       	/Eacute       	/Ecircumflex  	/Edieresis    	
/Igrave       	/Iacute       	/Icircumflex  	/Idieresis    	
/Eth          	/Ntilde       	/Ograve       	/Oacute       	
/Ocircumflex  	/Otilde       	/Odieresis    	/multiply     	
/Oslash       	/Ugrave       	/Uacute       	/Ucircumflex  	
/Udieresis    	/Yacute       	/Thorn        	/germandbls   	
/agrave       	/aacute       	/acircumflex  	/atilde       	
/adieresis    	/aring        	/ae           	/ccedilla     	
/egrave       	/eacute       	/ecircumflex  	/edieresis    	
/igrave       	/iacute       	/icircumflex  	/idieresis    	
/eth          	/ntilde       	/ograve       	/oacute       	
/ocircumflex  	/otilde       	/odieresis    	/divide       	
/oslash       	/ugrave       	/uacute       	/ucircumflex  	
/udieresis    	/yacute       	/thorn        	/ydieresis    	
] def
%%EndResource
%%EndProlog
%%BeginSetup
%%IncludeResource: font Courier-Bold
%%IncludeResource: font Courier
/HFpt_w 10 def
/HFpt_h 10 def
/Courier-Bold /HF-gs-font MF
/HF /HF-gs-font findfont [HFpt_w 0 0 HFpt_h 0 0] makefont def
/Courier /F-gs-font MF
/F-gs-font 10 10 SF
/#copies 1 def
% Pagedevice definitions:
gs_languagelevel 1 gt {
  <<
    /PageSize [595 842] 
  >> setpagedevice
} if
/d_page_w 559 def
/d_page_h 770 def
/d_header_x 0 def
/d_header_y 770 def
/d_header_w 559 def
/d_header_h 0 def
/d_footer_x 0 def
/d_footer_y 0 def
/d_footer_w 559 def
/d_footer_h 0 def
/d_output_w 559 def
/d_output_h 770 def
/cols 1 def
%%EndSetup
%%Page: (1) 1
%%BeginPageSetup
_S
18 36 translate
/pagenum 1 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 724 M
(Network Working Group                                           S. Barre) s
5 713 M
(Internet-Draft                                                 C. Paasch) s
5 702 M
(Expires: September 8, 2011                                O. Bonaventure) s
5 691 M
(                                                      UCLouvain, Belgium) s
5 680 M
(                                                           March 7, 2011) s
5 647 M
(              MultiPath TCP - Guidelines for implementers) s
5 636 M
(                       draft-barre-mptcp-impl-00) s
5 614 M
(Abstract) s
5 592 M
(   Multipath TCP is a major extension to TCP that allows improving the) s
5 581 M
(   resource usage in the current Internet by transmitting data over) s
5 570 M
(   several TCP subflows, while still showing one single regular TCP) s
5 559 M
(   socket to the application.  This document describes our experience in) s
5 548 M
(   writing a MultiPath TCP implementation in the Linux kernel and) s
5 537 M
(   discusses implementation guidelines that could be useful for other) s
5 526 M
(   developers who are planning to add MultiPath TCP to their networking) s
5 515 M
(   stack.) s
5 493 M
(Status of this Memo) s
5 471 M
(   This Internet-Draft is submitted in full conformance with the) s
5 460 M
(   provisions of BCP 78 and BCP 79.) s
5 438 M
(   Internet-Drafts are working documents of the Internet Engineering) s
5 427 M
(   Task Force \(IETF\).  Note that other groups may also distribute) s
5 416 M
(   working documents as Internet-Drafts.  The list of current Internet-) s
5 405 M
(   Drafts is at http://datatracker.ietf.org/drafts/current/.) s
5 383 M
(   Internet-Drafts are draft documents valid for a maximum of six months) s
5 372 M
(   and may be updated, replaced, or obsoleted by other documents at any) s
5 361 M
(   time.  It is inappropriate to use Internet-Drafts as reference) s
5 350 M
(   material or to cite them other than as "work in progress.") s
5 328 M
(   This Internet-Draft will expire on September 8, 2011.) s
5 306 M
(Copyright Notice) s
5 284 M
(   Copyright \(c\) 2011 IETF Trust and the persons identified as the) s
5 273 M
(   document authors.  All rights reserved.) s
5 251 M
(   This document is subject to BCP 78 and the IETF Trust's Legal) s
5 240 M
(   Provisions Relating to IETF Documents) s
5 229 M
(   \(http://trustee.ietf.org/license-info\) in effect on the date of) s
5 218 M
(   publication of this document.  Please review these documents) s
5 207 M
(   carefully, as they describe your rights and restrictions with respect) s
5 163 M
(Barre, et al.           Expires September 8, 2011               [Page 1]) s
_R
S
%%Page: (2) 2
%%BeginPageSetup
_S
18 36 translate
/pagenum 2 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   to this document.  Code Components extracted from this document must) s
5 702 M
(   include Simplified BSD License text as described in Section 4.e of) s
5 691 M
(   the Trust Legal Provisions and are provided without warranty as) s
5 680 M
(   described in the Simplified BSD License.) s
5 647 M
(Table of Contents) s
5 625 M
(   1.  Introduction . . . . . . . . . . . . . . . . . . . . . . . . .  3) s
5 614 M
(     1.1.  Terminology  . . . . . . . . . . . . . . . . . . . . . . .  3) s
5 603 M
(   2.  An architecture for Multipath transport  . . . . . . . . . . .  5) s
5 592 M
(     2.1.  MPTCP architecture . . . . . . . . . . . . . . . . . . . .  5) s
5 581 M
(     2.2.  Structure of the Multipath Transport . . . . . . . . . . .  9) s
5 570 M
(     2.3.  Structure of the Path Manager  . . . . . . . . . . . . . .  9) s
5 559 M
(   3.  MPTCP challenges for the OS  . . . . . . . . . . . . . . . . . 12) s
5 548 M
(     3.1.  Charging the application for its CPU cycles  . . . . . . . 12) s
5 537 M
(     3.2.  At connection/subflow establishment  . . . . . . . . . . . 13) s
5 526 M
(     3.3.  Subflow management . . . . . . . . . . . . . . . . . . . . 14) s
5 515 M
(     3.4.  At the data sink . . . . . . . . . . . . . . . . . . . . . 14) s
5 504 M
(       3.4.1.  Receive buffer tuning  . . . . . . . . . . . . . . . . 15) s
5 493 M
(       3.4.2.  Receive queue management . . . . . . . . . . . . . . . 15) s
5 482 M
(       3.4.3.  Scheduling data ACKs . . . . . . . . . . . . . . . . . 16) s
5 471 M
(     3.5.  At the data source . . . . . . . . . . . . . . . . . . . . 16) s
5 460 M
(       3.5.1.  Send buffer tuning . . . . . . . . . . . . . . . . . . 17) s
5 449 M
(       3.5.2.  Send queue management  . . . . . . . . . . . . . . . . 17) s
5 438 M
(       3.5.3.  Scheduling data  . . . . . . . . . . . . . . . . . . . 20) s
5 427 M
(         3.5.3.1.  The congestion controller  . . . . . . . . . . . . 21) s
5 416 M
(         3.5.3.2.  The Packet Scheduler . . . . . . . . . . . . . . . 22) s
5 405 M
(     3.6.  At connection/subflow termination  . . . . . . . . . . . . 23) s
5 394 M
(   4.  Configuring the OS for MPTCP . . . . . . . . . . . . . . . . . 25) s
5 383 M
(     4.1.  Source address based routing . . . . . . . . . . . . . . . 25) s
5 372 M
(     4.2.  Buffer configuration . . . . . . . . . . . . . . . . . . . 27) s
5 361 M
(   5.  Future work  . . . . . . . . . . . . . . . . . . . . . . . . . 28) s
5 350 M
(   6.  Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . 29) s
5 339 M
(   7.  References . . . . . . . . . . . . . . . . . . . . . . . . . . 30) s
5 328 M
(   Appendix A.  Design alternatives . . . . . . . . . . . . . . . . . 32) s
5 317 M
(     A.1.  Another way to consider Path Management  . . . . . . . . . 32) s
5 306 M
(     A.2.  Implementing alternate Path Managers . . . . . . . . . . . 33) s
5 295 M
(     A.3.  When to instantiate a new meta-socket ?  . . . . . . . . . 34) s
5 284 M
(     A.4.  Forcing more processing in user context  . . . . . . . . . 34) s
5 273 M
(     A.5.  Buffering data on a per-subflow basis  . . . . . . . . . . 35) s
5 262 M
(   Appendix B.  Ongoing discussions on implementation improvements  . 39) s
5 251 M
(     B.1.  Heuristics for subflow management  . . . . . . . . . . . . 39) s
5 240 M
(   Authors' Addresses . . . . . . . . . . . . . . . . . . . . . . . . 41) s
5 152 M
(Barre, et al.           Expires September 8, 2011               [Page 2]) s
_R
S
%%Page: (3) 3
%%BeginPageSetup
_S
18 36 translate
/pagenum 3 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(1.  Introduction) s
5 691 M
(   The MultiPath TCP protocol [1] is a major TCP extension that allows) s
5 680 M
(   for simultaneous use of multiple paths, while being transparent to) s
5 669 M
(   the applications, fair to regular TCP flows [2] and deployable in the) s
5 658 M
(   current Internet.  The MPTCP design goals and the protocol) s
5 647 M
(   architecture that allow reaching them are described in [3].  Besides) s
5 636 M
(   the protocol architecture, a number of non-trivial design choices) s
5 625 M
(   need to be made in order to extend an existing TCP implementation to) s
5 614 M
(   support MultiPath TCP.  This document gathers a set of guidelines) s
5 603 M
(   that should help implementers writing an efficient and modular MPTCP) s
5 592 M
(   stack.  The guidelines are expected to be applicable regardless of) s
5 581 M
(   the Operating System \(although the MPTCP implementation described) s
5 570 M
(   here is done in Linux [4]\).  Another goal is to achieve the greatest) s
5 559 M
(   level of modularity without impacting efficiency, hence allowing) s
5 548 M
(   other multipath protocols to nicely co-exist in the same stack.  In) s
5 537 M
(   order for the reader to clearly disambiguate "useful hints" from) s
5 526 M
(   "important requirements", we write the latter in their own) s
5 515 M
(   paragraphs, starting with the keyword "IMPORTANT".  By important) s
5 504 M
(   requirements, we mean design options that, if not followed, would) s
5 493 M
(   lead to an under-performing MPTCP stack, maybe even slower than) s
5 482 M
(   regular TCP.) s
5 460 M
(   This draft presents implementation guidelines that are based on the) s
5 449 M
(   code which has been implemented in our MultiPath TCP aware Linux) s
5 438 M
(   kernel \(the version covered here is 0.6\) which is available from) s
5 427 M
(   http://inl.info.ucl.ac.be/mptcp.  We also list configuration) s
5 416 M
(   guidelines that have proven to be useful in practice.  In some cases,) s
5 405 M
(   we discuss some mechanisms that have not yet been implemented.  These) s
5 394 M
(   mechanisms are clearly listed.  During our work in implementing) s
5 383 M
(   MultiPath TCP, we evaluated other designs.  Some of them are not used) s
5 372 M
(   anymore in our implementation.  However, we explain in the appendix) s
5 361 M
(   the reason why these particular designs have not been considered) s
5 350 M
(   further.) s
5 328 M
(   This document is structured as follows.  First we propose an) s
5 317 M
(   architecture that allows supporting MPTCP in a protocol stack) s
5 306 M
(   residing in an operating system.  Then we consider a range of) s
5 295 M
(   problems that must be solved by an MPTCP stack \(compared to a regular) s
5 284 M
(   TCP stack\).  In Section 4, we propose recommendations on how a system) s
5 273 M
(   administrator could correctly configure an MPTCP-enabled host.) s
5 262 M
(   Finally, we discuss future work, in particular in the area of MPTCP) s
5 251 M
(   optimization.) s
5 229 M
(1.1.  Terminology) s
5 207 M
(   In this document we use the same terminology as in [3] and [1].  In) s
5 196 M
(   addition, we will use the following implementation-specific terms:) s
5 152 M
(Barre, et al.           Expires September 8, 2011               [Page 3]) s
_R
S
%%Page: (4) 4
%%BeginPageSetup
_S
18 36 translate
/pagenum 4 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   o  Meta-socket: A socket structure used to reorder incoming data at) s
5 702 M
(      the connection level and schedule outgoing data to subflows.) s
5 680 M
(   o  Master subsocket: The socket structure that is visible from the) s
5 669 M
(      application.  If regular TCP is in use, this is the only active) s
5 658 M
(      socket structure.  If MPTCP is used, this is the socket) s
5 647 M
(      corresponding to the first subflow.) s
5 625 M
(   o  Slave subsocket: Any socket created by the kernel to provide an) s
5 614 M
(      additional subflow.  Those sockets are not visible to the) s
5 603 M
(      application \(unless a specific API [5] is used\).  The meta-socket,) s
5 592 M
(      master and slave subsocket are explained in more details in) s
5 581 M
(      Section 2.2.) s
5 559 M
(   o  Endpoint id: Endpoint identifier.  It is the tuple \(saddr, sport,) s
5 548 M
(      daddr, dport\) that identifies a particular subflow, hence a) s
5 537 M
(      particular subsocket.) s
5 515 M
(   o  Fendpoint id: First Endpoint identifier.  It is the endpoint) s
5 504 M
(      identifier of the Master subsocket.) s
5 482 M
(   o  Connection id or token: It is a locally unique number, defined in) s
5 471 M
(      Section 2 of [1], that allows finding a connection during the) s
5 460 M
(      establishment of new subflows.) s
5 152 M
(Barre, et al.           Expires September 8, 2011               [Page 4]) s
_R
S
%%Page: (5) 5
%%BeginPageSetup
_S
18 36 translate
/pagenum 5 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(2.  An architecture for Multipath transport) s
5 691 M
(   Section 4 of the MPTCP architecture document [3] describes the) s
5 680 M
(   functional decomposition of MPTCP.  It lists four entities, namely) s
5 669 M
(   Path Management, Packet Scheduling, Subflow Interface and Congestion) s
5 658 M
(   Control.  These entities can be further grouped based on the layer at) s
5 647 M
(   which they operate:) s
5 625 M
(   o  Transport layer: This includes Packet Scheduling, Subflow) s
5 614 M
(      Interface and Congestion Control, and is grouped under the term) s
5 603 M
(      "Multipath Transport \(MT\)".  From an implementation point of view,) s
5 592 M
(      they all will involve modifications to TCP.) s
5 570 M
(   o  Any layer: Path Management.  Path management can be done in the) s
5 559 M
(      transport layer, as is the case of the built-in path manager \(PM\)) s
5 548 M
(      described in [1].  That PM discovers paths through the exchange of) s
5 537 M
(      TCP options of type ADD_ADDR or the reception of a SYN on a new) s
5 526 M
(      address pair, and defines a path as an endpoint_id \(saddr, sport,) s
5 515 M
(      daddr, dport\).  But, more generally, a PM could be any module able) s
5 504 M
(      to expose multiple paths to MPTCP, located either in kernel or) s
5 493 M
(      user space, and acting on any OSI layer \(e.g. a bonding driver) s
5 482 M
(      that would expose its multiple links to the Multipath Transport\).) s
5 460 M
(   Because of the fundamental independence of Path Management compared) s
5 449 M
(   to the three other entities, we draw a clear line between both, and) s
5 438 M
(   define a simple interface that allows MPTCP to benefit easily from) s
5 427 M
(   any appropriately interfaced multipath technology.  In this document,) s
5 416 M
(   we stick to describing how the functional elements of MPTCP are) s
5 405 M
(   defined, using the built-in Path Manager described in [1], and we) s
5 394 M
(   leave for future separate documents the description of other path) s
5 383 M
(   managers.  We describe in the first subsection the precise roles of) s
5 372 M
(   the Multipath Transport and the Path Manager.  Then we detail how) s
5 361 M
(   they are interfaced with each other.) s
5 339 M
(2.1.  MPTCP architecture) s
5 317 M
(   Although, when using the built-in PM, MPTCP is fully contained in the) s
5 306 M
(   transport layer, it can still be organized as a Path Manager and a) s
5 295 M
(   Multipath Transport Layer as shown in Figure 1.  The Path Manager) s
5 284 M
(   announces to the MultiPath Transport what paths can be used through) s
5 273 M
(   path indices for an MPTCP connection, identified by the fendpoint_id) s
5 262 M
(   \(first endpoint id\).  The fendpoint_id is the tuple \(saddr, sport,) s
5 251 M
(   daddr, dport\) seen by the application and uniquely identifies the) s
5 240 M
(   MPTCP connection \(an alternate way to identify the MPTCP connection) s
5 229 M
(   being the conn_id, which is a token as described in Section 2 of) s
5 218 M
(   [1]\).  The Path Manager maintains the mapping between the path_index) s
5 207 M
(   and an endpoint_id.  The endpoint_id is the tuple \(saddr, sport,) s
5 196 M
(   daddr, dport\) that is to be used for the corresponding path index.) s
5 152 M
(Barre, et al.           Expires September 8, 2011               [Page 5]) s
_R
S
%%Page: (6) 6
%%BeginPageSetup
_S
18 36 translate
/pagenum 6 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   Note that the fendpoint_id itself represents a path and is thus a) s
5 702 M
(   particular endpoint_id.  By convention, the fendpoint_id is always) s
5 691 M
(   represented as path index 1.  As explained in [3], Section 5.6, it is) s
5 680 M
(   not yet clear how an implementation should behave in the event of a) s
5 669 M
(   failure in the first subflow.  We expect, however, that the Master) s
5 658 M
(   subsocket should be kept in use as an interface with the application,) s
5 647 M
(   even if no data is transmitted anymore over it.  It also allows the) s
5 636 M
(   fendpoint_id to remain meaningful throughout the life of the) s
5 625 M
(   connection.  This behavior has yet to be tested and refined with) s
5 614 M
(   Linux MPTCP.) s
5 592 M
(   Figure 1 shows an example sequence of MT-PM interactions happening at) s
5 581 M
(   the beginning of an exchange.  When the MT starts a new connection) s
5 570 M
(   \(through an application connect\(\) or accept\(\)\), it can request the PM) s
5 559 M
(   to be updated about possible alternate paths for this new connection.) s
5 548 M
(   The PM can also spontaneously update the MT at any time \(normally) s
5 537 M
(   when the path set changes\).  This is step 1 in Figure 1.  In the) s
5 526 M
(   example, 4 paths can be used, hence 3 new ones.  Based on the update,) s
5 515 M
(   the MT can decide whether to establish new subflows, and how many of) s
5 504 M
(   them.  Here, the MT decides to establish one subflow only, and sends) s
5 493 M
(   a request for endpoint_id to the PM.  This is step 2.  In step 3, the) s
5 482 M
(   answer is given: <A2,B2,0,pB2>.  The source port is unspecified to) s
5 471 M
(   allow the MT ensure the unicity of the new endpoint_id, thanks to the) s
5 460 M
(   new_port\(\) primitive \(present in regular TCP as well\).  Note that) s
5 449 M
(   messages 1,2,3 need not be real messages and can be function calls) s
5 438 M
(   instead \(as is the case in Linux MPTCP\).) s
5 152 M
(Barre, et al.           Expires September 8, 2011               [Page 6]) s
_R
S
%%Page: (7) 7
%%BeginPageSetup
_S
18 36 translate
/pagenum 7 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(                               Control plane) s
5 702 M
(   +---------------------------------------------------------------+) s
5 691 M
(   |                     Multipath Transport \(MT\)                  |) s
5 680 M
(   +----------------------------------------------------|----------+) s
5 669 M
(     ^                  |           ^                   v) s
5 658 M
(     |                  |           |          [Build new subsocket,) s
5 647 M
(     | 1.For fendpt_id  |2.endpt_id |                 with endpt_ids) s
5 636 M
(     |<A1,B1,pA1,pB1>   | for path  | 3.<A2,B2,   <A2,B2,new_port\(\),pB2]) s
5 625 M
(     |Paths 1->4 can be | index 2 ? |   0,pB2>) s
5 614 M
(     |used.             |           |) s
5 603 M
(     |                  |           |) s
5 592 M
(     |                  |           |) s
5 581 M
(     |                  v           |) s
5 570 M
(   +---------------------------------------------------------------+) s
5 559 M
(   |                         Path Manager \(PM\)                     |) s
5 548 M
(   +---------------------------------------------------------------+) s
5 537 M
(      /                                     \\) s
5 526 M
(     /---------------------------------------\\) s
5 515 M
(     | mapping table:                        |) s
5 504 M
(     |   Subflow   <--> endpoint_id          |) s
5 493 M
(     |  path index                           |) s
5 482 M
(     |                                       |) s
5 471 M
(     |    [see table below]                  |) s
5 460 M
(     |                                       |) s
5 449 M
(     +---------------------------------------+) s
5 416 M
(      Figure 1: Functional separation of MPTCP in the transport layer) s
5 394 M
(   The following options, described in [1] , are managed by the) s
5 383 M
(   Multipath Transport:) s
5 361 M
(   o  MULTIPATH CAPABLE \(MP_CAPABLE\): Tells the peer that we support) s
5 350 M
(      MPTCP and announces our local token.) s
5 328 M
(   o  MP_JOIN/MP_AUTH: Initiates a new subflow \(Note that MP_AUTH is not) s
5 317 M
(      yet part of our Linux implementation at the moment\)) s
5 295 M
(   o  DATA SEQUENCE NUMBER \(DSN_MAP\): Identifies the position of a set) s
5 284 M
(      of bytes in the meta-flow.) s
5 262 M
(   o  DATA_ACK: Acknowledge data at the connection level \(subflow level) s
5 251 M
(      acknowledgments are contained in the normal TCP header\).) s
5 229 M
(   o  DATA FIN \(DFIN\): Terminates a connection.) s
5 207 M
(   o  MP_PRIO: Asks the peer to revise the backup status of the subflow) s
5 196 M
(      on which the option is sent.  Although the option is sent by the) s
5 152 M
(Barre, et al.           Expires September 8, 2011               [Page 7]) s
_R
S
%%Page: (8) 8
%%BeginPageSetup
_S
18 36 translate
/pagenum 8 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(      Multipath Transport \(because this allows using the TCP option) s
5 702 M
(      space\), it may be triggered by the Path Manager.  This option is) s
5 691 M
(      not yet supported by our MPTCP implementation.) s
5 669 M
(   o  MP_FAIL: Checksum failed at connection-level.  Currently the Linux) s
5 658 M
(      implementation does not implement the checksum in option DSN_MAP,) s
5 647 M
(      and hence does not implement either the MP_FAIL option.) s
5 625 M
(   The Path manager applies a particular technology to give the MT the) s
5 614 M
(   possibility to use several paths.  The built-in MPTCP Path Manager) s
5 603 M
(   uses multiple IPv4/v6 addresses as its mean to influence the) s
5 592 M
(   forwarding of packets through the Internet.  When the MT starts a new) s
5 581 M
(   connection, it chooses a token that will be used to identify the) s
5 570 M
(   connection.  This is necessary to allow future subflow-establishment) s
5 559 M
(   SYNs \(that is, containing the MP_JOIN option\) to be attached to the) s
5 548 M
(   correct connection.  An example mapping table is given hereafter:) s
5 526 M
(                 +---------+------------+---------------+) s
5 515 M
(                 |  token  | path index |  Endpoint id  |) s
5 504 M
(                 +---------+------------+---------------+) s
5 493 M
(                 | token_1 |      1     | <A1,B1,0,pB1> |) s
5 482 M
(                 |         |            |               |) s
5 471 M
(                 | token_1 |      2     | <A2,B2,0,pB1> |) s
5 460 M
(                 |         |            |               |) s
5 449 M
(                 | token_1 |      3     | <A1,B2,0,pB1> |) s
5 438 M
(                 |         |            |               |) s
5 427 M
(                 | token_1 |      4     | <A2,B1,0,pB1> |) s
5 416 M
(                 |         |            |               |) s
5 405 M
(                 |         |            |               |) s
5 394 M
(                 | token_2 |      1     | <A1,B1,0,pB2> |) s
5 383 M
(                 |         |            |               |) s
5 372 M
(                 | token_2 |      2     | <A2,B1,0,pB2> |) s
5 361 M
(                 +---------+------------+---------------+) s
5 339 M
(              Table 1: Example mapping table for built-in PM) s
5 317 M
(   Table 1 shows an example where two MPTCP connections are active.  One) s
5 306 M
(   is identified by token_1, the other one with token_2.  As per [1],) s
5 295 M
(   the tokens must be unique locally.  Since the endpoint identifier may) s
5 284 M
(   change from one subflow to another, the attachment of incoming new) s
5 273 M
(   subflows \(identified by a SYN + MP_JOIN option\) to the right) s
5 262 M
(   connection is achieved thanks to the locally unique token.  The) s
5 251 M
(   built-in path manager currently implements the following options The) s
5 240 M
(   following options \(defined in [1]\) are intended to be part of the) s
5 229 M
(   built-in path manager:) s
5 207 M
(   o  Add Address \(ADD_ADDR\): Announces a new address we own) s
5 152 M
(Barre, et al.           Expires September 8, 2011               [Page 8]) s
_R
S
%%Page: (9) 9
%%BeginPageSetup
_S
18 36 translate
/pagenum 9 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   o  Remove Address \(REMOVE_ADDR\): Withdraws a previously announced) s
5 702 M
(      address) s
5 680 M
(   Those options form the built-in MPTCP Path Manager, based on) s
5 669 M
(   declaring IP addresses, and carries control information in TCP) s
5 658 M
(   options.  An implementation of Multipath TCP can use any Path) s
5 647 M
(   Manager, but it must be able to fallback to the default PM in case) s
5 636 M
(   the other end does not support the custom PM.  Alternative Path) s
5 625 M
(   Managers may be specified in separate documents in the future.) s
5 603 M
(2.2.  Structure of the Multipath Transport) s
5 581 M
(   The Multipath Transport handles three kinds of sockets.  We define) s
5 570 M
(   them here and use this notation throughout the entire document:) s
5 548 M
(   o  Master subsocket: This is the first socket in use when a) s
5 537 M
(      connection \(TCP or MPTCP\) starts.  It is also the only one in use) s
5 526 M
(      if we need to fall back to regular TCP.  This socket is initiated) s
5 515 M
(      by the application through the socket\(\) system call.  Immediately) s
5 504 M
(      after a new master subsocket is created, MPTCP capability is) s
5 493 M
(      enabled by the creation of the meta-socket.) s
5 471 M
(   o  Meta-socket: It holds the multipath control block, and acts as the) s
5 460 M
(      connection level socket.  As data source, it holds the main send) s
5 449 M
(      buffer.  As data sink, it holds the connection-level receive queue) s
5 438 M
(      and out-of-order queue \(used for reordering\).  We represent it as) s
5 427 M
(      a normal \(extended\) socket structure in Linux MPTCP because this) s
5 416 M
(      allows reusing much of the existing TCP code with few) s
5 405 M
(      modifications.  In particular, the regular socket structure) s
5 394 M
(      already holds pointers to SND.UNA, SND.NXT, SND.WND, RCV.NXT,) s
5 383 M
(      RCV.WND \(as defined in [6]\).  It also holds all the necessary) s
5 372 M
(      queues for sending/receiving data.) s
5 350 M
(   o  Slave subsocket: Any subflow created by MPTCP, in addition to the) s
5 339 M
(      first one \(the master subsocket is always considered as a subflow) s
5 328 M
(      even though it may be in failed state at some point in the) s
5 317 M
(      communication\).  The slave subsockets are created by the kernel) s
5 306 M
(      \(not visible from the application\) The master subsocket and the) s
5 295 M
(      slave subsockets together form the pool of available subflows that) s
5 284 M
(      the MPTCP Packet Scheduler \(called from the meta-socket\) can use) s
5 273 M
(      to send packets.) s
5 251 M
(2.3.  Structure of the Path Manager) s
5 229 M
(   In contrast to the multipath transport, which is more complex and) s
5 218 M
(   divided in sub-entities \(namely Packet Scheduler, Subflow Interface) s
5 207 M
(   and Congestion Control, see Section 2\), the Path Manager just) s
5 196 M
(   maintains the mapping table and updates the Multipath Transport when) s
5 152 M
(Barre, et al.           Expires September 8, 2011               [Page 9]) s
_R
S
%%Page: (10) 10
%%BeginPageSetup
_S
18 36 translate
/pagenum 10 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   the mapping table changes.  The mapping table has been described) s
5 702 M
(   above \(Table 1\).  We detail in Table 2 the set of \(event,action\)) s
5 691 M
(   pairs that are implemented in the Linux MPTCP built-in path manager.) s
5 680 M
(   For reference, an earlier architecture for the Path Management is) s
5 669 M
(   discussed in Appendix A.1.  Also, Appendix A.2 proposes a small) s
5 658 M
(   extension to this current architecture to allow supporting other path) s
5 647 M
(   managers.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 10]) s
_R
S
%%Page: (11) 11
%%BeginPageSetup
_S
18 36 translate
/pagenum 11 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   +-------------------------+-----------------------------------------+) s
5 702 M
(   | event                   | action                                  |) s
5 691 M
(   +-------------------------+-----------------------------------------+) s
5 680 M
(   | master_sk bound: This   | Discovers the set of local addresses    |) s
5 669 M
(   | event is triggered upon | and stores them in local_addr_table     |) s
5 658 M
(   | either a bind\(\),        |                                         |) s
5 647 M
(   | connect\(\), or when a    |                                         |) s
5 636 M
(   | new server-side socket  |                                         |) s
5 625 M
(   | becomes established.    |                                         |) s
5 614 M
(   |                         |                                         |) s
5 603 M
(   | ADD_ADDR option         | Updates remote_addr_table               |) s
5 592 M
(   | received or SYN+MP_JOIN | correspondingly                         |) s
5 581 M
(   | received on new address |                                         |) s
5 570 M
(   |                         |                                         |) s
5 559 M
(   | local/remote_addr_table | Updates mapping_table by adding any new |) s
5 548 M
(   | updated                 | address combinations, or removing the   |) s
5 537 M
(   |                         | ones that have disappeared. Each        |) s
5 526 M
(   |                         | address pair is given a path index.     |) s
5 515 M
(   |                         | Once allocated to an address pair, a    |) s
5 504 M
(   |                         | path index cannot be reallocated to     |) s
5 493 M
(   |                         | another one, to ensure consistency of   |) s
5 482 M
(   |                         | the mapping table.                      |) s
5 471 M
(   |                         |                                         |) s
5 460 M
(   | Mapping_table updated   | Sends notification to the Multipath     |) s
5 449 M
(   |                         | Transport. The notification contains    |) s
5 438 M
(   |                         | the new set of path indices that the MT |) s
5 427 M
(   |                         | is allowed to use. This is shown in     |) s
5 416 M
(   |                         | Figure 1, msg 1.                        |) s
5 405 M
(   |                         |                                         |) s
5 394 M
(   | Endpoint_id\(path_index\) | Retrieves the endpoint_ids for the      |) s
5 383 M
(   | request received from   | corresponding path index from the       |) s
5 372 M
(   | MT \(Figure 1, msg 2\)    | mapping table and returns them to the   |) s
5 361 M
(   |                         | MT. One such request/response is        |) s
5 350 M
(   |                         | illustrated in Figure 1, msg 3. Note    |) s
5 339 M
(   |                         | that in that msg 3, the local port is   |) s
5 328 M
(   |                         | set to zero. This is to let the         |) s
5 317 M
(   |                         | operating system choose a unique local  |) s
5 306 M
(   |                         | port for the new socket.                |) s
5 295 M
(   +-------------------------+-----------------------------------------+) s
5 273 M
(       Table 2: \(event,action\) pairs implemented in the  built-in PM) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 11]) s
_R
S
%%Page: (12) 12
%%BeginPageSetup
_S
18 36 translate
/pagenum 12 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(3.  MPTCP challenges for the OS) s
5 691 M
(   MPTCP is a major modification to the MPTCP stack.  We have described) s
5 680 M
(   above an architecture that separates Multipath Transport from Path) s
5 669 M
(   Management.  Path Management can be implemented rather simply.  But) s
5 658 M
(   Multipath Transport involves a set of new challenges, that do not) s
5 647 M
(   exist in regular TCP.  We first describe how an MPTCP client or) s
5 636 M
(   server can start a new connection, or a new subflow within a) s
5 625 M
(   connection.  Then we propose techniques \(a concrete implementation of) s
5 614 M
(   which is done in Linux MPTCP\) to efficiently implement data reception) s
5 603 M
(   \(at the data sink\) and data sending \(at the data source\).) s
5 581 M
(3.1.  Charging the application for its CPU cycles) s
5 559 M
(   As this document is about implementation, it is important not only to) s
5 548 M
(   ensure that MPTCP is fast, but also that it is fair to other) s
5 537 M
(   applications that share the same CPU.  Otherwise one could have an) s
5 526 M
(   extremely fast file transfer, while the rest of the system is just) s
5 515 M
(   hanging.  CPU fairness is ensured by the scheduler of the Operating) s
5 504 M
(   System when things are implemented in user space.  But in the kernel,) s
5 493 M
(   we can choose to run code in "user context", that is, in a mode where) s
5 482 M
(   each CPU cycle is charged to a particular application.  Or we can) s
5 471 M
(   \(and must in some cases\) run code in "interrupt context", that is,) s
5 460 M
(   interrupting everything else until the task has finished.  In Linux) s
5 449 M
(   \(probably a similar thing is true in other systems\), the arrival of a) s
5 438 M
(   new packet on a NIC triggers a hardware interrupt, which in turn) s
5 427 M
(   schedules a software interrupt that will pull the packet from the NIC) s
5 416 M
(   and perform the initial processing.  The challenge is to stop the) s
5 405 M
(   processing of the incoming packet in software interrupt as soon as it) s
5 394 M
(   can be attached to a socket, and wake up the application.  With TCP,) s
5 383 M
(   an additional constraint is that incoming data should be acknowledged) s
5 372 M
(   as soon as possible, which requires reordering.  Van Jacobson has) s
5 361 M
(   proposed a solution for this [7]: If an application is waiting on a) s
5 350 M
(   recv\(\) system call, incoming packets can be put into a special queue) s
5 339 M
(   \(called prequeue in Linux\) and the application is woken up.) s
5 328 M
(   Reordering and acknowledgement are then performed in user context.) s
5 317 M
(   The execution path for outgoing packets is less critical from that) s
5 306 M
(   point of view, because the vast majority of processing can be done) s
5 295 M
(   very easily in user context.) s
5 273 M
(   In this document, when discussing CPU fairness, we will use the) s
5 262 M
(   following terms:) s
5 240 M
(   o  User context: Execution environment that is under control of the) s
5 229 M
(      OS scheduler.  CPU cycles are charged to the associated) s
5 218 M
(      application, which allows to ensure fairness with other) s
5 207 M
(      applications.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 12]) s
_R
S
%%Page: (13) 13
%%BeginPageSetup
_S
18 36 translate
/pagenum 13 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   o  Interrupt context: Execution environment that runs with higher) s
5 702 M
(      priority than any process.  Although it is impossible to) s
5 691 M
(      completely avoid running code in interrupt context, it is) s
5 680 M
(      important to minimize the amount of code running in such a) s
5 669 M
(      context.) s
5 647 M
(   o  VJ prequeues: This refers to Van Jacobson prequeues, as explained) s
5 636 M
(      above[7].) s
5 614 M
(3.2.  At connection/subflow establishment) s
5 592 M
(   As described in [1], the establishment of an MPTCP connection is) s
5 581 M
(   quite simple, being just a regular three-way exchange with additional) s
5 570 M
(   options.  As shown in Section 2.2 this is done in the master) s
5 559 M
(   subsocket.  Currently Linux MPTCP attaches a meta-socket to a socket) s
5 548 M
(   as soon as it is created, that is, upon a socket\(\) system call) s
5 537 M
(   \(client side\), or when a server side socket enters the ESTABLISHED) s
5 526 M
(   state.  An alternate solution is described in Appendix A.3.) s
5 504 M
(   An implementation can choose the best moment, maybe depending on the) s
5 493 M
(   OS, to instantiate the meta-socket.  However, if this meta-socket is) s
5 482 M
(   needed to accept new subflows \(like it is in Linux MPTCP\), it should) s
5 471 M
(   be attached at the latest when the MP_CAPABLE option is received.) s
5 460 M
(   Otherwise incoming new subflow requests \(SYN + MP_JOIN\) may be lost,) s
5 449 M
(   requiring retransmissions by the peer and delaying the subflow) s
5 438 M
(   establishment.) s
5 416 M
(   The establishment of subflows, on the other hand, is more tricky.) s
5 405 M
(   The problem is that new SYNs \(with the MP_JOIN option\) must be) s
5 394 M
(   accepted by a socket \(the meta-socket in the proposed design\) as if) s
5 383 M
(   it was in LISTEN state, while its state is actually ESTABLISHED.) s
5 372 M
(   There is the following in common with a LISTEN socket:) s
5 350 M
(   o  Temporary structure: Between the reception of the SYN and the) s
5 339 M
(      final ACK, a mini-socket is used as a temporary structure.) s
5 317 M
(   o  Queue of connection requests: The meta-socket, like a LISTEN) s
5 306 M
(      socket, maintains a list of pending connection requests.  There) s
5 295 M
(      are two such lists.  One contains mini-sockets, because the final) s
5 284 M
(      ACK has not yet been received.  The second list contains sockets) s
5 273 M
(      in the ESTABLISHED state that have not yet been accepted.) s
5 262 M
(      "Accepted" means, for regular TCP, returned to the application as) s
5 251 M
(      a result of an accept\(\) system call.  For MPTCP it means that the) s
5 240 M
(      new subflow has been integrated in the set of active subflows.) s
5 218 M
(   We can list the following differences with a normal LISTEN socket.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 13]) s
_R
S
%%Page: (14) 14
%%BeginPageSetup
_S
18 36 translate
/pagenum 14 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   o  Socket lookup for a SYN: When a SYN is received, the corresponding) s
5 702 M
(      LISTEN socket is found by using the endpoint_id.  This is not) s
5 691 M
(      possible with MPTCP, since we can receive a SYN on any) s
5 680 M
(      endpoint_id.  Instead, the token must be used to retrieve the) s
5 669 M
(      meta-socket to which the SYN must be attached.  A new hashtable) s
5 658 M
(      must be defined, with tokens as keys.) s
5 636 M
(   o  Lookup for connection request: In regular TCP, this lookup is) s
5 625 M
(      quite similar to the previous one \(in Linux at least\).  The) s
5 614 M
(      5-tuple is used, first to find the LISTEN socket, next to retrieve) s
5 603 M
(      the corresponding mini-socket, stored in a private hashtable) s
5 592 M
(      inside the LISTEN socket.  With MPTCP, we cannot do that, because) s
5 581 M
(      there is no way to retrieve the meta-socket from the final ACK.) s
5 570 M
(      The 5-tuple can be anything, and the token is only present in the) s
5 559 M
(      SYN.  There is no token in the final ACK.  Our Linux MPTCP) s
5 548 M
(      implementation uses a global hashtable for pending connection) s
5 537 M
(      requests, where the key is the 5-tuple of the connection request.) s
5 515 M
(   An implementation must carefully check the presence of the MP_JOIN) s
5 504 M
(   option in incoming SYNs before performing the usual socket lookup.) s
5 493 M
(   If it is present, only the token-based lookup must be done.  If this) s
5 482 M
(   lookup does not return a meta-socket, the SYN must be discarded.) s
5 471 M
(   Failing to do that could lead to mistakenly attach the incoming SYN) s
5 460 M
(   to a LISTEN socket instead of attaching it to a meta-socket.) s
5 438 M
(3.3.  Subflow management) s
5 416 M
(   Further research is needed to define the appropriate heuristics to) s
5 405 M
(   solve these problems.  Initial thoughts are provided in Appendix B.1.) s
5 383 M
(   Currently, in a Linux MPTCP client, the Multipath Transport tries to) s
5 372 M
(   open all subflows advertised by the Path Manager.  On the other hand,) s
5 361 M
(   the server only accepts new subflows, but does not try to establish) s
5 350 M
(   new ones.  The rationale for this is that the client is the) s
5 339 M
(   connection initiator.  New subflows are only established if the) s
5 328 M
(   initiator requests them.  This is subject to change in future) s
5 317 M
(   releases of our MPTCP implementation.) s
5 295 M
(3.4.  At the data sink) s
5 273 M
(   There is a symmetry between the behavior of the data source and the) s
5 262 M
(   data sink.  Yet, the specific requirements are different.  The data) s
5 251 M
(   sink is described in this section while the data source is described) s
5 240 M
(   in the next section.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 14]) s
_R
S
%%Page: (15) 15
%%BeginPageSetup
_S
18 36 translate
/pagenum 15 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(3.4.1.  Receive buffer tuning) s
5 691 M
(   The MPTCP required receive buffer is larger than the sum of the) s
5 680 M
(   buffers required by the individual subflows.  The reason for this and) s
5 669 M
(   proper values for the buffer are explained in [3] Section 5.3.  Not) s
5 658 M
(   following this could result in the MPTCP speed being capped at the) s
5 647 M
(   bandwidth of the slowest subflow.) s
5 625 M
(   An interesting way to dynamically tune the receive buffer according) s
5 614 M
(   the bandwidth/delay product \(BDP\) of a path, for regular TCP, is) s
5 603 M
(   described in [8] and implemented in recent Linux kernels.  It uses) s
5 592 M
(   the COPIED_SEQ sequence variable \(sequence number of the next byte to) s
5 581 M
(   copy to the app buffer\) to count, every RTT, the number of bytes) s
5 570 M
(   received during that RTT.  This number of bytes is precisely the BDP.) s
5 559 M
(   The accuracy of this technique is directly dependent on the accuracy) s
5 548 M
(   of the RTT estimation.  Unfortunately, the data sink does not have a) s
5 537 M
(   reliable estimate of the SRTT.  To solve this, [8] proposes two) s
5 526 M
(   techniques:) s
5 504 M
(   1.  Using the timestamp option \(quite accurate\).) s
5 482 M
(   2.  Computing the time needed to receive one RCV.WND [6] worth of) s
5 471 M
(       data.  It is less precise and is used only to compute an upper) s
5 460 M
(       bound on the required receive buffer.) s
5 438 M
(   As described in [1], section 3.3.3, the MPTCP advertised receive) s
5 427 M
(   window is shared by all subflows.  Hence, no per-subflow information) s
5 416 M
(   can be deduced from it, and the second technique from [8] cannot be) s
5 405 M
(   used. [3] mentions that the allocated connection-level receive buffer) s
5 394 M
(   should be 2*sum\(BW_i\)*RTT_max, where BW_i is the bandwidth seen by) s
5 383 M
(   subflow i and RTT_max is the maximum RTT estimated among all the) s
5 372 M
(   subflows.  This is achieved in Linux MPTCP by slightly modifying the) s
5 361 M
(   first tuning algorithm from [8], and disabling the second one.  The) s
5 350 M
(   modification consists in counting on each subflow, every RTT_max the) s
5 339 M
(   number of bytes received during that time on this subflow.  Per) s
5 328 M
(   subflow, this provides its contribution to the total receive buffer) s
5 317 M
(   of the connection.  This computes the contribution of each subflow to) s
5 306 M
(   the total receive buffer of the connection.) s
5 284 M
(3.4.2.  Receive queue management) s
5 262 M
(   As advised in [1], Section 3.3.1, "subflow-level processing should be) s
5 251 M
(   undertaken separately from that at connection-level".  This also has) s
5 240 M
(   the side-effect of allowing much code reuse from the regular TCP) s
5 229 M
(   stack.  A regular TCP stack \(in Linux at least\) maintains a receive) s
5 218 M
(   queue \(for storing incoming segments until the application asks for) s
5 207 M
(   them\) and an out-of-order queue \(to allow reordering\).  In Linux) s
5 196 M
(   MPTCP, the subflow-level receive-queue is not used.  Incoming) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 15]) s
_R
S
%%Page: (16) 16
%%BeginPageSetup
_S
18 36 translate
/pagenum 16 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   segments are reordered at the subflow-level, just as if they were) s
5 702 M
(   plain TCP data.  But once the data is in-order at the subflow level,) s
5 691 M
(   it can be immediately handed to MPTCP \(See Figure 7 of [3]\) for) s
5 680 M
(   connection-level reordering.  The role of the subflow-level receive) s
5 669 M
(   queue is now taken by the MPTCP-level receive queue.  In order to) s
5 658 M
(   maximize the CPU cycles spent in user context \(see Section 3.1\), VJ) s
5 647 M
(   prequeues can be used just as in regular TCP \(they are not yet) s
5 636 M
(   supported in Linux MPTCP, though\).) s
5 614 M
(   An alternate design, where the subflow-level receive queue is kept) s
5 603 M
(   active and the MPTCP receive queue is not used, is discussed in) s
5 592 M
(   Appendix A.4.) s
5 570 M
(3.4.3.  Scheduling data ACKs) s
5 548 M
(   As specified in [1], Section 3.3.2, data ACKs not only help the) s
5 537 M
(   sender in having a consistent view of what data has been correctly) s
5 526 M
(   received at the connection level.  They are also used as the left) s
5 515 M
(   edge of the advertised receive window.) s
5 493 M
(   In regular TCP, if a receive buffer becomes full, the receiver) s
5 482 M
(   announces a receive window.  When finally some bytes are given to the) s
5 471 M
(   application, freeing space in the receive buffer, a duplicate ACK is) s
5 460 M
(   sent to act as a window upate, so that the sender knows it can) s
5 449 M
(   transmit again.  Likewise, when the MPTCP shared receive buffer) s
5 438 M
(   becomes full, a zero window is advertised.  When some bytes are) s
5 427 M
(   delivered to the application, a duplicate DATA_ACK must be sent to) s
5 416 M
(   act as a window update.  Such an important DATA_ACK should be sent on) s
5 405 M
(   all subflows, to maximize the probability that at least one of them) s
5 394 M
(   reaches the peer.  If, however, all DATA_ACKs are lost, there is no) s
5 383 M
(   other option than relying on the window probes periodically sent by) s
5 372 M
(   the data source, as in regular TCP.) s
5 350 M
(   In theory a DATA_ACK can be sent on any subflow, or even on all) s
5 339 M
(   subflows, simultaneously.  As of version 0.5, Linux MPTCP simply adds) s
5 328 M
(   the DATA_ACK option to any outgoing segment \(regardless of whether it) s
5 317 M
(   is data or a pure ACK\).  There is thus no particular DATA_ACK) s
5 306 M
(   scheduling policy.  The only exception is for a window update that) s
5 295 M
(   follows a zero-window.  In this case, the behavior is as described in) s
5 284 M
(   the previous paragraph.) s
5 262 M
(3.5.  At the data source) s
5 240 M
(   In this section we mirror the topics of the previous section, in the) s
5 229 M
(   case of a data sender.  The sender does not have the same view of the) s
5 218 M
(   communication, because one has information that the other can only) s
5 207 M
(   estimate.  Also, the data source sends data and receives) s
5 196 M
(   acknowledgements, while the data sink does the reverse.  This results) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 16]) s
_R
S
%%Page: (17) 17
%%BeginPageSetup
_S
18 36 translate
/pagenum 17 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   in a different set of problems to be dealt with by the data source.) s
5 691 M
(3.5.1.  Send buffer tuning) s
5 669 M
(   As explained in [3], end of Section 5.3, the send buffer should have) s
5 658 M
(   the same size as the receive buffer.  At the sender, we don't have) s
5 647 M
(   the RTT estimation problem described in Section 3.4.1, because we can) s
5 636 M
(   reuse the built-in TCP SRTT \(smoothed RTT\).  Moreover, the sender has) s
5 625 M
(   the congestion window, which is itself an estimate of the BDP, and is) s
5 614 M
(   used in Linux to tune the send buffer of regular TCP.  Unfortunately,) s
5 603 M
(   we cannot use the congestion window with MPTCP, because the buffer) s
5 592 M
(   equation does not involve the product BW_i*delay_i for the subflows) s
5 581 M
(   \(which is what the congestion window estimates\), but it involves) s
5 570 M
(   BW_i*delay_max, where delay_max is the maximum observed delay across) s
5 559 M
(   all subflows.  An obvious way to compute the contribution of each) s
5 548 M
(   subflow to the receive buffer would be: 2*\(cwnd_i/SRTT_i\)*SRTT_max.) s
5 537 M
(   However, some care is needed because of the variability of the SRTT) s
5 526 M
(   \(measurements show that, even smoothed, the SRTT is not quite) s
5 515 M
(   stable\).  Currently Linux MPTCP estimates the bandwidth periodically) s
5 504 M
(   by checking the sequence number progress.  This however introduces) s
5 493 M
(   new mechanisms in the kernel, that could probably be avoided.  Future) s
5 482 M
(   experience will tell what is appropriate.) s
5 460 M
(3.5.2.  Send queue management) s
5 438 M
(   As MultiPath TCP involves the use of several TCP subflows, a) s
5 427 M
(   scheduler must be added to decide where to send each byte of data.) s
5 416 M
(   Two possible places for the scheduler have been evaluated for Linux) s
5 405 M
(   MPTCP.  One option is to schedule data as soon as it arrives from the) s
5 394 M
(   application buffer.  This option, consisting in _pushing_ data to) s
5 383 M
(   subflows as soon as it is available, was implemented in older) s
5 372 M
(   versions of Linux MPTCP and is now abandoned.  We keep a description) s
5 361 M
(   of it \(and why it has been abandoned\) in Appendix A.5.  Another) s
5 350 M
(   option is to store all data centrally in the Multipath Transport,) s
5 339 M
(   inside a shared send buffer \(see Figure 2\).  Scheduling is then done) s
5 328 M
(   at transmission time, whenever any subflow becomes ready to send more) s
5 317 M
(   data \(usually due to acknowledgements having opened space in the) s
5 306 M
(   congestion window\).  In that scenario, the subflows _pull_ segments) s
5 295 M
(   from the shared send queue whenever they are ready.  Note that) s
5 284 M
(   several subflows can become ready simultaneously, if an) s
5 273 M
(   acknowledgement advertises a new receive window, that opens more) s
5 262 M
(   space in the shared send window.  For that reason, when a subflow) s
5 251 M
(   pulls data, the Packet Scheduler is run and other subflows may be fed) s
5 240 M
(   by the Packet Scheduler in the same time.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 17]) s
_R
S
%%Page: (18) 18
%%BeginPageSetup
_S
18 36 translate
/pagenum 18 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(                               Application) s
5 702 M
(                                   |) s
5 691 M
(                                   v) s
5 680 M
(                                 | * |) s
5 669 M
(    Next segment to send \(A\)  -> | * |) s
5 658 M
(                                 |---| <- Shared send queue) s
5 647 M
(   Sent, but not DATA-acked\(B\)-> |_*_|) s
5 636 M
(                                   |) s
5 625 M
(                                   v) s
5 614 M
(                             Packet Scheduler) s
5 603 M
(                                  /  \\) s
5 592 M
(                                 /    \\) s
5 581 M
(                                |      |) s
5 570 M
(                                v      v) s
5 559 M
(   Sent, but not acked\(B\)  ->  |_|    |_| <- Subflow level congestion) s
5 548 M
(                                |      |     window) s
5 537 M
(                                v      v) s
5 526 M
(                               NIC    NIC) s
5 504 M
(                    Figure 2: Send queue configuration) s
5 482 M
(   This approach, similar to the one proposed in [9], presents several) s
5 471 M
(   advantages:) s
5 449 M
(   o  Each subflow can easily fill its pipe.  \(As long as there is data) s
5 438 M
(      to pull from the shared send buffer, and the scheduler is not) s
5 427 M
(      applying a policy that restricts the subflow\).) s
5 405 M
(   o  If a subflow fails, it will no longer receive acknowledgements,) s
5 394 M
(      and hence will naturally stop pulling from the shared send buffer.) s
5 383 M
(      This removes the need for an explicit "failed state", to ensure) s
5 372 M
(      that a failed subflow does not receive data \(As opposed to e.g.) s
5 361 M
(      SCTP-CMT, that needs an explicit marking of failed subflows by) s
5 350 M
(      design, because it uses a single sequence number space [10]\).) s
5 328 M
(   o  Similarly, when a failed subflow becomes active again, the pending) s
5 317 M
(      segments of its congestion window are finally acknowledged,) s
5 306 M
(      allowing it to pull again from the shared send buffer.  Note that) s
5 295 M
(      in such a case, the acknowledged data is normally just dropped by) s
5 284 M
(      the receiver, because the corresponding segments have been) s
5 273 M
(      retransmitted on another subflow during the failure time.) s
5 251 M
(   Despite the adoption of that approach in Linux MPTCP, there are still) s
5 240 M
(   two drawbacks:) s
5 218 M
(   o  There is one single queue, in the Multipath Transport, from which) s
5 207 M
(      all subflows pull segments.  In Linux, queue processing is) s
5 196 M
(      optimized for handling segments, not bytes.  This implies that the) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 18]) s
_R
S
%%Page: (19) 19
%%BeginPageSetup
_S
18 36 translate
/pagenum 19 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(      shared send queue must contain pre-built segments, hence requiring) s
5 702 M
(      the _same_ MSS to be used for all subflows.  We note however that) s
5 691 M
(      today, the most commonly negotiated MSS is around 1380 bytes [4],) s
5 680 M
(      so this approach sounds reasonable.  Should this requirement) s
5 669 M
(      become too constraining in the future, a more flexible approach) s
5 658 M
(      could be devised \(e.g., supporting a few Maximum Segment Sizes\).) s
5 636 M
(   o  Because the subflows pull data whenever they get new free space in) s
5 625 M
(      their congestion window, the Packet Scheduler must run at that) s
5 614 M
(      time.  But that time most often corresponds to the reception of an) s
5 603 M
(      acknowledgement, which happens in interrupt context \(see) s
5 592 M
(      Section 3.1\).  This is both unfair to other system processes, and) s
5 581 M
(      slightly inefficient for high speed communications.  The problem) s
5 570 M
(      is that the packet scheduler performs more operations that the) s
5 559 M
(      usual "copy packet to NIC".  One way to solve this problem would) s
5 548 M
(      be to have a small subflow-specific send queue, which would) s
5 537 M
(      actually lead to a hybrid architecture between the pull approach) s
5 526 M
(      \(described here\) and the push approach \(described in) s
5 515 M
(      Appendix A.5\).  Doing that would require solving non-trivial) s
5 504 M
(      problems, though, and requires further study.) s
5 482 M
(   As shown, in Figure 2, a segment first enters the shared send queue,) s
5 471 M
(   then, when reaching the bottom of that queue, it is pulled by some) s
5 460 M
(   subflow.  But to support failures, we need to be able to move) s
5 449 M
(   segments from one subflow to another, so that the failure is) s
5 438 M
(   invisible from the application.  In Linux MPTCP, the segment data is) s
5 427 M
(   kept in the Shared send queue \(B portion of the queue\).  When a) s
5 416 M
(   subflow pulls a segment, it actually only copies the control) s
5 405 M
(   structure \(struct sk_buff\) \(which Linux calls packet cloning\) and) s
5 394 M
(   increments its reference count.  The following event/action table) s
5 383 M
(   summarizes these operations:) s
5 361 M
(   +-----------------+-------------------------------------------------+) s
5 350 M
(   | event           | action                                          |) s
5 339 M
(   +-----------------+-------------------------------------------------+) s
5 328 M
(   | Segment         | Remove references to the segment from the       |) s
5 317 M
(   | acknowledged at | subflow-level queue                             |) s
5 306 M
(   | subflow level   |                                                 |) s
5 295 M
(   |                 |                                                 |) s
5 284 M
(   | Segment         | Remove references to the segment from the       |) s
5 273 M
(   | acknowledged at | connection-level queue                          |) s
5 262 M
(   | connection      |                                                 |) s
5 251 M
(   | level           |                                                 |) s
5 240 M
(   |                 |                                                 |) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 19]) s
_R
S
%%Page: (20) 20
%%BeginPageSetup
_S
18 36 translate
/pagenum 20 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   | Timeout         | Push the segment to the best running subflow    |) s
5 702 M
(   | \(subflow-level\) | \(according to the Packet Scheduler\). If no      |) s
5 691 M
(   |                 | subflow is available, push it to a temporary    |) s
5 680 M
(   |                 | retransmit queue \(not represented in Figure 2\)  |) s
5 669 M
(   |                 | for future pulling by an available subflow. The |) s
5 658 M
(   |                 | retransmit queue is parallel to the connection  |) s
5 647 M
(   |                 | level queue and is read with higher priority.   |) s
5 636 M
(   |                 |                                                 |) s
5 625 M
(   | Ready to put    | If the retransmit queue is not empty, first     |) s
5 614 M
(   | new data on the | pull from there. Otherwise, then take new       |) s
5 603 M
(   | wire \(normally  | segment\(s\) from the connection level send queue |) s
5 592 M
(   | triggered by an | \(A portion\). The pulling operation is a bit     |) s
5 581 M
(   | incoming ack\)   | special in that it can result in sending a      |) s
5 570 M
(   |                 | segment over a different subflow than the one   |) s
5 559 M
(   |                 | which initiated the pull. This is because the   |) s
5 548 M
(   |                 | Packet Scheduler is run as part of the pull,    |) s
5 537 M
(   |                 | which can result in selecting any subflow. In   |) s
5 526 M
(   |                 | most cases, though, the subflow which           |) s
5 515 M
(   |                 | originated the pull will get fresh data, given  |) s
5 504 M
(   |                 | it has space for that in the congestion window. |) s
5 493 M
(   |                 | Note that the subflows have no A portion in     |) s
5 482 M
(   |                 | Figure 2, because they immediately send the     |) s
5 471 M
(   |                 | data they pull.                                 |) s
5 460 M
(   +-----------------+-------------------------------------------------+) s
5 438 M
(   Table 3: \(event,action\) pairs implemented in the  Multipath Transport) s
5 427 M
(                             queue management) s
5 405 M
(   IMPORTANT: A subflow can be stopped from transmitting by the) s
5 394 M
(   congestion window, but also by the send window \(that is, the receive) s
5 383 M
(   window announced by the peer\).  Given that the receive window has a) s
5 372 M
(   connection level meaning, a DATA_ACK arriving on one subflow could) s
5 361 M
(   unblock another subflow.  Implementations should be aware of this to) s
5 350 M
(   avoid stalling part of the subflows in such situations.  In the case) s
5 339 M
(   of Linux MPTCP, that follows the above architecture, this is ensured) s
5 328 M
(   by running the Packet Scheduler at each pull operation.  This is not) s
5 317 M
(   completely optimal, though, and may be revised when more experience) s
5 306 M
(   is gained.) s
5 284 M
(3.5.3.  Scheduling data) s
5 262 M
(   As several subflows may be used to transmit data, MPTCP must select a) s
5 251 M
(   subflow to send each data.  First, we need to know which subflows are) s
5 240 M
(   available for sending data.  The mechanism that controls this is the) s
5 229 M
(   congestion controller, which maintains a per-subflow congestion) s
5 218 M
(   window.  The aim of a Multipath congestion controller is to move data) s
5 207 M
(   away from congested links, and ensure fairness when there is a shared) s
5 196 M
(   bottleneck.  The handling of the congestion window is explained in) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 20]) s
_R
S
%%Page: (21) 21
%%BeginPageSetup
_S
18 36 translate
/pagenum 21 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   Section 3.5.3.1.  Given a set of available subflows \(according to the) s
5 702 M
(   congestion window\), one of these has to be selected by the Packet) s
5 691 M
(   Scheduler.  The role of the Packet Scheduler is to implement a) s
5 680 M
(   particular policy, as will be explained in Section 3.5.3.2.) s
5 658 M
(3.5.3.1.  The congestion controller) s
5 636 M
(   The Coupled Congestion Control provided in Linux MPTCP implements the) s
5 625 M
(   algorithm defined in [2].  Operating System kernels \(Linux at least\)) s
5 614 M
(   do not support floating-point numbers for efficiency reasons. [2]) s
5 603 M
(   makes an extensive use of them, which must be worked around.  Linux) s
5 592 M
(   MPTCP solves that by performing fixed-point operations using a) s
5 581 M
(   minimum number of fractions and performs scaling when divisions are) s
5 570 M
(   necessary.) s
5 548 M
(   Linux already includes a work-around for floating point operations in) s
5 537 M
(   the Reno congestion avoidance implementation.  Upon reception of an) s
5 526 M
(   ack, the congestion window \(counted in segments, not in bytes as) s
5 515 M
(   proposed in [2] does\) should be updated as cwnd+=1/cwnd.  Instead,) s
5 504 M
(   Linux increments the separate variable snd_cwnd_cnt, until) s
5 493 M
(   snd_cwnd_cnt>=cwnd.  When this happens, snd_cwnd_cnt is reset, and) s
5 482 M
(   cwnd is incremented.  Linux MPTCP reuses this to update the window in) s
5 471 M
(   the CCC \(Coupled Congestion Control\) congestion avoidance phase:) s
5 460 M
(   snd_cwnd_cnt is incremented as previously explained, and cwnd is) s
5 449 M
(   incremented when snd_cwnd_cnt >= max\(tot_cwnd / alpha, cwnd\) \(see) s
5 438 M
(   [2]\).  Note that the bytes_acked variable, present in [2], is not) s
5 427 M
(   included here because Linux MPTCP does not currently support ABC) s
5 416 M
(   [11], but instead considers acknowledgements in MSS units.  Linux) s
5 405 M
(   uses for ABC, in Reno, the bytes_acked variable instead of) s
5 394 M
(   snd_cwnd_cnt.  For Reno, cwnd is incremented by one if) s
5 383 M
(   bytes_acked>=cwnd*MSS.  Hence, in the case of a CCC with ABC, one) s
5 372 M
(   would increment cwnd when bytes_acked>=max\(tot_cwnd*MSS / alpha,) s
5 361 M
(   cwnd*MSS\).) s
5 339 M
(   Unfortunately, the alpha parameter mentioned above involves many) s
5 328 M
(   fractions.  The current implementation of MPTCP uses a rewritten) s
5 317 M
(   version of the alpha formula from [2]:) s
5 295 M
(                             cwnd_max * scale_num) s
5 284 M
(   alpha = tot_cwnd * ----------------------------------) s
5 273 M
(                     /     rtt_max * cwnd_i * scale_den \\ 2) s
5 262 M
(                     | sum -----------------------------|) s
5 251 M
(                     \\  i              rtt_i            /) s
5 229 M
(   This computation assumes that the MSS is shared by all subflows,) s
5 218 M
(   which is true under the architecture described in Section 3.5.2 but) s
5 207 M
(   implies that implementations choosing to support several MSS cannot) s
5 196 M
(   use the above simplified equation.  The variables cwnd_max and) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 21]) s
_R
S
%%Page: (22) 22
%%BeginPageSetup
_S
18 36 translate
/pagenum 22 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   rtt_max in the above equation are NOT resp. the maximum congestion) s
5 702 M
(   window and RTT across all subflows.  Instead, they are the values of) s
5 691 M
(   subflow i such that cwnd_i / rtt_i^2 is maximum.  This corresponds to) s
5 680 M
(   the numerator of the equation provided in [2].) s
5 658 M
(   scale_num and scale_den have to be selected in such a way that) s
5 647 M
(   scale_num > scale_den^2.  A good choice is to use scale_num=2^32) s
5 636 M
(   \(using 64 bits arithmetic\) and scale_den=2^10.  In that case the) s
5 625 M
(   final alpha value is scaled by 2^12, which gives a reasonable) s
5 614 M
(   precision.  Due to the scaling, it is necessary to also scale later) s
5 603 M
(   in the formula that decides whether an increase of the congestion) s
5 592 M
(   window is necessary or not: snd_cwnd_cnt >= max\(\(tot_cwnd<<12\) /) s
5 581 M
(   alpha,cwnd\).) s
5 559 M
(3.5.3.2.  The Packet Scheduler) s
5 537 M
(   Whenever the Congestion Controller \(described above\) allows new data) s
5 526 M
(   for at least one subflow, the Packet Scheduler is run.  When only one) s
5 515 M
(   subflow is available the Packet Scheduler just decides which packet) s
5 504 M
(   to pick from the A section of the shared send buffer \(see Figure 2\).) s
5 493 M
(   Currently Linux MPTCP picks the bottom most segment.  If more than) s
5 482 M
(   one subflow is available, there are three decisions to take:) s
5 460 M
(   o  Which of the subflows to feed with fresh data: As the only Packet) s
5 449 M
(      Scheduler currently supported in Linux MPTCP aims at filling all) s
5 438 M
(      pipes, it always feeds data to all subflows as long as there is) s
5 427 M
(      data to send.) s
5 405 M
(   o  In what order to feed selected subflows: when several subflows) s
5 394 M
(      become available simultaneously, they are fed by order of time-) s
5 383 M
(      distance to the client.  We define the time-distance as the time) s
5 372 M
(      needed for the packet to reach the peer if given to a particular) s
5 361 M
(      subflow.  This time depends on the RTT, bandwidth and queue size) s
5 350 M
(      \(in bytes\), as follows: time_distance_i = queue_size_i/bw_i+RTT_i.) s
5 339 M
(      Given that with the architecture described in Section 3.5.2, the) s
5 328 M
(      subflow-specific queue size cannot exceed a congestion window, the) s
5 317 M
(      time_distance becomes time_distance_i~=RTT_i.  This scheduling) s
5 306 M
(      policy favors fast subflows for application-limited communications) s
5 295 M
(      \(where all subflows need not be used\).  However, for network-) s
5 284 M
(      limited communications, this scheduling policy has little effect) s
5 273 M
(      because all subflows will be used at some point, even the slow) s
5 262 M
(      ones, to try minimizing the connection-level completion time.) s
5 240 M
(   o  How much data to allocate to a single subflow: this question) s
5 229 M
(      concerns the granularity of the allocation.  Using big allocation) s
5 218 M
(      units allows for better support of TCP Segmentation Offload \(TSO\).) s
5 207 M
(      TSO allows the system to aggregate several times the MSS into one) s
5 196 M
(      single segment, sparing memory and CPU cycles, by leaving the) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 22]) s
_R
S
%%Page: (23) 23
%%BeginPageSetup
_S
18 36 translate
/pagenum 23 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(      fragmentation task to the NIC.  However, this is only possible if) s
5 702 M
(      the large single segment is made of contiguous data, at the) s
5 691 M
(      subflow level and the connection level \(see also important note) s
5 680 M
(      below\).) s
5 658 M
(   IMPORTANT: When scheduling data to subflows, an implementation must) s
5 647 M
(   be careful that if two segments are contiguous at the subflow-level,) s
5 636 M
(   but non-contiguous at the connection level, they cannot be aggregated) s
5 625 M
(   into one.  As Linux \(and probably other systems\) merges segments when) s
5 614 M
(   it is under memory pressure, it could easily decide to merge non-) s
5 603 M
(   contiguous MPTCP segments, simply because they look contiguous from) s
5 592 M
(   the subflow viewpoint.  This must be avoided, because the DATA_SEQ) s
5 581 M
(   mapping option would loose its meaning in such a case, leading to all) s
5 570 M
(   possible kinds of misbehaviors.) s
5 548 M
(3.6.  At connection/subflow termination) s
5 526 M
(   In Linux MPTCP, subflows are terminated only when the whole) s
5 515 M
(   connection terminates, because the heuristic for terminating subflows) s
5 504 M
(   \(without closing the connection\) is not yet mature, as explained in) s
5 493 M
(   Section 3.3.) s
5 471 M
(   At connection termination, an implementation must ensure that all) s
5 460 M
(   subflows plus the meta-socket are cleanly removed.  The obvious) s
5 449 M
(   choice to propagate the close\(\) system call on all subflows does not) s
5 438 M
(   work.  The problem is that a close\(\) on a subflow appends a FIN at) s
5 427 M
(   the end of the send queue.  If we transpose this to the meta-socket,) s
5 416 M
(   we would append a DATA_FIN on the shared send queue \(see) s
5 405 M
(   Section 3.5.2\).  That operation results in the shared send queue not) s
5 394 M
(   accepting any more data from the application, which is correct.  It) s
5 383 M
(   also results in the subflow-specific queues not accepting any more) s
5 372 M
(   data from the shared send queue.  The shared send queue may however) s
5 361 M
(   still be full of segments, which will never be sent because all gates) s
5 350 M
(   are closed.) s
5 328 M
(   IMPORTANT: Upon a close\(\) system call, an implementation must refrain) s
5 317 M
(   from sending a FIN on all subflows, unless the implementation uses an) s
5 306 M
(   architecture with no connection-level send queue \(like the one) s
5 295 M
(   described in Appendix A.5\).  Even in that case, it makes sense to) s
5 284 M
(   keep all subflows open until the last byte is sent, to allow) s
5 273 M
(   retransmission on any path, should any one of them fail.) s
5 251 M
(   Currently, upon a close\(\) system call, Linux MPTCP appends a DATA_FIN) s
5 240 M
(   to the connection-level send queue.  Only when that DATA_FIN reaches) s
5 229 M
(   the bottom of the send queue is the regular FIN sent on all subflows.) s
5 207 M
(   DISCUSSION: In the Linux MPTCP behavior described above, a connection) s
5 196 M
(   could still stall near its end if one path fails while transmitting) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 23]) s
_R
S
%%Page: (24) 24
%%BeginPageSetup
_S
18 36 translate
/pagenum 24 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   its last congestion window of data \(because the maximum size of the) s
5 702 M
(   subflow-specific send queue is cwnd\).  This can be avoided by waiting) s
5 691 M
(   just a bit more before to trigger the subflow-FIN: Instead of sending) s
5 680 M
(   the FIN together with the DATA_FIN, send the DATA_FIN alone and wait) s
5 669 M
(   for the corresponding DATA_ACK to trigger a FIN on all subflows.) s
5 658 M
(   This however augments by one RTT the duration of the overall) s
5 647 M
(   connection termination.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 24]) s
_R
S
%%Page: (25) 25
%%BeginPageSetup
_S
18 36 translate
/pagenum 25 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(4.  Configuring the OS for MPTCP) s
5 691 M
(   Previous sections concentrated on implementations.  In this section,) s
5 680 M
(   we try to gather guidelines that help getting the full potential from) s
5 669 M
(   MPCTP through appropriate system configuration.  Currently those) s
5 658 M
(   guidelines apply especially to Linux, but the principles can be) s
5 647 M
(   applied to other systems.) s
5 625 M
(4.1.  Source address based routing) s
5 603 M
(   As already pointed out by [12], the default behavior of most) s
5 592 M
(   operating systems is not appropriate for the use of multiple) s
5 581 M
(   interfaces.  Most operating systems are typically configured to use) s
5 570 M
(   at most one IP address at a time.  It is more and more common to) s
5 559 M
(   maintain several links in up state \(e.g. using the wired interface as) s
5 548 M
(   main link, but maintaining a ready-to-use wireless link in the) s
5 537 M
(   background, to facilitate fallback when the wired link fails\).  But) s
5 526 M
(   MPTCP is not about that.  MPTCP is about _simultaneously_ using) s
5 515 M
(   several interfaces \(when available\).  It is expected that one of the) s
5 504 M
(   mostly used MPTCP configurations will be through two or more NICs,) s
5 493 M
(   each being assigned a different address.  Another possible) s
5 482 M
(   configuration would be to assign several IP addresses to the same) s
5 471 M
(   interface, in which case the path diverges later in the network,) s
5 460 M
(   based on the particular address that is used in the packet.) s
5 438 M
(   Usually an operating system has a single default route, with a single) s
5 427 M
(   source IP address.  If the host has several IP addresses and we want) s
5 416 M
(   to do MultiPath TCP, it is necessary to configure source address) s
5 405 M
(   based routing.  This means that based on the source address, selected) s
5 394 M
(   by the MultiPath TCP-module in the operating system, the routing-) s
5 383 M
(   decision is based on a different routing table.  Each of these) s
5 372 M
(   routing tables defines a default route to the Internet.  This is) s
5 361 M
(   different from defining several default routes in the same routing) s
5 350 M
(   table \(which is also supported in Linux\), because in that case only) s
5 339 M
(   the first one is used.  Any additional default route is considered as) s
5 328 M
(   a fallback route, used only in case the main one fails.) s
5 306 M
(   It is easier to understand the necessary configuration by means of an) s
5 295 M
(   example.  Let a host have two interfaces,I1 and I2, both connected to) s
5 284 M
(   the public Internet and being assigned addresses resp. A1 and A2.) s
5 273 M
(   Such a host needs 3 routing tables.  One of them is the classical) s
5 262 M
(   default routing table, present in all systems.  The default routing) s
5 251 M
(   table is used to find a route based on the destination address only,) s
5 240 M
(   when a segment is issued with the undetermined source address.  The) s
5 229 M
(   undetermined source address is typically used by applications that) s
5 218 M
(   initiate a TCP connect\(\) system call, specifying the destination) s
5 207 M
(   address but letting the system choose the source address.  In that) s
5 196 M
(   case, after the default routing table has been consulted, an address) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 25]) s
_R
S
%%Page: (26) 26
%%BeginPageSetup
_S
18 36 translate
/pagenum 26 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   is assigned to the socket by the system by applying [13].  The) s
5 702 M
(   additional routing tables are used when the source address is) s
5 691 M
(   specified.  If the source address has no impact on the route that) s
5 680 M
(   should be chosen, then the default routing table is sufficient.  But) s
5 669 M
(   this is a particular case \(e.g., a host connected to one network) s
5 658 M
(   only, but using two addresses to exploit ECMP paths later in the) s
5 647 M
(   network\).  In most cases, a source address is attached to a specific) s
5 636 M
(   interface, or at least a specific gateway.  Both of those cases) s
5 625 M
(   require defining a separate routing table, one per \(gateway, outgoing) s
5 614 M
(   interface\) pair.  To select the proper routing table based on the) s
5 603 M
(   source address, an additional indirection level must be configured.) s
5 592 M
(   It is called "policy routing" in Linux and is illustrated at the) s
5 581 M
(   bottom of Figure 3.) s
5 559 M
(   +----------------------------------------------------+) s
5 548 M
(   |                   Default Table                    |) s
5 537 M
(   +----------------------------------------------------+) s
5 526 M
(   | Dst: 0.0.0.0/0  Via: Gateway-IP1 Dev: I1           |) s
5 515 M
(   | Dst: 0.0.0.0/0  Via: Gateway-IP2 Dev: I2           |) s
5 504 M
(   | Dst: Gateway1-Subnet Dev: I1 Src: A1  Scope: Link  |) s
5 493 M
(   | Dst: Gateway2-Subnet Dev: I2 Src: A2  Scope: Link  |) s
5 482 M
(   +----------------------------------------------------+) s
5 460 M
(   +----------------------------------------------------+) s
5 449 M
(   |                      Table 1                       |) s
5 438 M
(   +----------------------------------------------------+) s
5 427 M
(   | Dst: 0.0.0.0/0  Via: Gateway-IP1 Dev: I1           |) s
5 416 M
(   | Dst: Gateway1-Subnet Dev: I1 Src: A1 Scope: Link   |) s
5 405 M
(   +----------------------------------------------------+) s
5 383 M
(   +----------------------------------------------------+) s
5 372 M
(   |                      Table 2                       |) s
5 361 M
(   +----------------------------------------------------+) s
5 350 M
(   | Dst: 0.0.0.0/0  Via: Gateway-IP2 Dev: I2           |) s
5 339 M
(   | Dst: Gateway2-Subnet Dev: I2 Src: A2 Scope: Link   |) s
5 328 M
(   +----------------------------------------------------+) s
5 306 M
(   +----------------------------------------------------+) s
5 295 M
(   |                 Policy Table                       |) s
5 284 M
(   +----------------------------------------------------+) s
5 273 M
(   |   If src == A1 , Table 1                           |) s
5 262 M
(   |   If src == A2 , Table 2                           |) s
5 251 M
(   +----------------------------------------------------+) s
5 229 M
(          Figure 3: Routing table configuration for MultiPath TCP) s
5 207 M
(   If only the default routing table were used, only the first default) s
5 196 M
(   route would be used, regardless of the source address.  For example,) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 26]) s
_R
S
%%Page: (27) 27
%%BeginPageSetup
_S
18 36 translate
/pagenum 27 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   a packet with source address A2, would leave the host through) s
5 702 M
(   interface I1, which is incorrect.) s
5 680 M
(4.2.  Buffer configuration) s
5 658 M
(   [3], Section 5.3 describes in details the new, higher buffer) s
5 647 M
(   requirements of MPTCP.  Section 3.4.1 and Section 3.5.1 describe how) s
5 636 M
(   the MPTCP buffers can be tuned dynamically.  However, it is important) s
5 625 M
(   to note that even the best tuning is capped by a maximum configured) s
5 614 M
(   at the system level.  When using MultiPath TCP, the maximum receive) s
5 603 M
(   and send buffer should be configured to a higher value than for) s
5 592 M
(   regular TCP.  There is no universal guideline on what value is best) s
5 581 M
(   there.  Instead the most appropriate action, for an administrator, is) s
5 570 M
(   probably to roughly estimate the maximum bandwidth and delay that can) s
5 559 M
(   be observed on a particular connectivity setup, and apply the) s
5 548 M
(   equation from [3], Section 5.3 to find a reasonable tradeoff.  This) s
5 537 M
(   exercise could lead an administrator to decide to disable MPTCP on) s
5 526 M
(   some interfaces, because it allows consuming less memory while still) s
5 515 M
(   achieving reasonable performance.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 27]) s
_R
S
%%Page: (28) 28
%%BeginPageSetup
_S
18 36 translate
/pagenum 28 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(5.  Future work) s
5 691 M
(   A lot of work has yet to be done, and there is much space for) s
5 680 M
(   improvements.  In this section we try to assemble a list of future) s
5 669 M
(   improvements that would complete this guidelines.) s
5 647 M
(   o  Today's host processors have more and more CPU cores.  Given) s
5 636 M
(      Multipath TCP tries to exploit another form of parallelism, there) s
5 625 M
(      is a challenge in finding how those they can work together) s
5 614 M
(      optimally.  An important question is how to work with hardware) s
5 603 M
(      that behaves intelligently with TCP \(e.g. flow to core affinity\).) s
5 592 M
(      This problem is discussed in more details in [14].) s
5 570 M
(   o  An evaluation of Linux MPTCP exists [4].  But many optimizations) s
5 559 M
(      are still possible and should be evaluated.  Examples of them VJ) s
5 548 M
(      prequeues \(Section 3.1\), MPTCP fast path \(that is, a translation) s
5 537 M
(      of the existing TCP fast path to MPTCP\) or DMA support.  VJ) s
5 526 M
(      prequeues, described in Section 3.1, are intended to defer segment) s
5 515 M
(      processing until the application is awoken, when possible.) s
5 493 M
(   o  Currently, support for TCP Segmentation Offload remains a) s
5 482 M
(      challenge because it plays with the Maximum Segment Size.  Linux) s
5 471 M
(      MPTCP currently works with a single MSS across all subflows \(see) s
5 460 M
(      Section 3.5.2\).  Adding TSO support to MPTCP is certainly) s
5 449 M
(      possible, but requires further work \(Section 3.5.2\).  Also,) s
5 438 M
(      support for Large Receive Offload has not been investigated yet.) s
5 416 M
(   o  There are ongoing discussions on heuristics that would be used to) s
5 405 M
(      decide when to start new subflows.  Those discussions are) s
5 394 M
(      summarized in Appendix B.1, but none of the proposed heuristics) s
5 383 M
(      have been evaluated yet.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 28]) s
_R
S
%%Page: (29) 29
%%BeginPageSetup
_S
18 36 translate
/pagenum 29 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(6.  Acknowledgements) s
5 691 M
(   Sebastien Barre, Christoph Paasch and Olivier Bonaventure are) s
5 680 M
(   supported by Trilogy \(http://www.trilogy-project.org\), a research) s
5 669 M
(   project \(ICT-216372\) partially funded by the European Community under) s
5 658 M
(   its Seventh Framework Program.  The views expressed here are those of) s
5 647 M
(   the author\(s\) only.  The European Commission is not liable for any) s
5 636 M
(   use that may be made of the information in this document.) s
5 614 M
(   The authors gratefully acknowledge Costin Raiciu, who wrote a) s
5 603 M
(   userland implementation of MPTCP and provided insight on) s
5 592 M
(   implementation matters during several fruitful debates.  Discussions) s
5 581 M
(   with Janardhan Iyengar also helped understanding the specificities of) s
5 570 M
(   MPTCP compared to SCTP-CMT.) s
5 548 M
(   The authors would also like to thank the following people for useful) s
5 537 M
(   discussions on the mailing list and/or reviews: Alan Ford, Bob) s
5 526 M
(   Briscoe, Mark Handley, Michael Scharf.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 29]) s
_R
S
%%Page: (30) 30
%%BeginPageSetup
_S
18 36 translate
/pagenum 30 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(7.  References) s
5 691 M
(   [1]   Ford, A., Raiciu, C., and M. Handley, "TCP Extensions for) s
5 680 M
(         Multipath Operation with Multiple Addresses",) s
5 669 M
(         draft-ietf-mptcp-multiaddressed-02 \(work in progress\),) s
5 658 M
(         October 2010.) s
5 636 M
(   [2]   Raiciu, C., Handley, M., and D. Wischik, "Coupled Congestion) s
5 625 M
(         Control for Multipath Transport Protocols",) s
5 614 M
(         draft-ietf-mptcp-congestion-01 \(work in progress\),) s
5 603 M
(         January 2011.) s
5 581 M
(   [3]   Ford, A., Raiciu, C., Handley, M., Barre, S., and J. Iyengar,) s
5 570 M
(         "Architectural Guidelines for Multipath TCP Development",) s
5 559 M
(         draft-ietf-mptcp-architecture-05 \(work in progress\),) s
5 548 M
(         January 2011.) s
5 526 M
(   [4]   Barre, S., Paasch, C., and O. Bonaventure, "Multipath TCP: From) s
5 515 M
(         Theory to Practice", IFIP Networking,Valencia , May 2011,) s
5 504 M
(         <http://inl.info.ucl.ac.be/publications/mptcp-nw>.) s
5 482 M
(   [5]   Scharf, M. and A. Ford, "MPTCP Application Interface) s
5 471 M
(         Considerations", draft-ietf-mptcp-api-00 \(work in progress\),) s
5 460 M
(         November 2010.) s
5 438 M
(   [6]   Postel, J., "Transmission Control Protocol", STD 7, RFC 793,) s
5 427 M
(         September 1981.) s
5 405 M
(   [7]   Jacobson, V., "Re: query about tcp header on tcp-ip", Sep 1993,) s
5 394 M
(         <ftp://ftp.ee.lbl.gov/email/vanj.93sep07.txt>.) s
5 372 M
(   [8]   Fisk, M. and W. Feng, "Dynamic right-sizing in TCP", Los Alamos) s
5 361 M
(         Computer Science Institute Symposium , 2001,) s
5 350 M
(         <http://woozle.org/~mfisk/papers/tcpwindow-lacsi.pdf>.) s
5 328 M
(   [9]   Hsieh, H. and R. Sivakumar, "pTCP: An End-to-End Transport) s
5 317 M
(         Layer Protocol for Striped Connections", ICNP , 2002, <http://) s
5 306 M
(         www.ece.gatech.edu/research/GNAN/archive/2002/icnp02h.html>.) s
5 284 M
(   [10]  Becke, M., Dreibholz, T., Iyengar, J., Natarajan, P., and M.) s
5 273 M
(         Tuexen, "Load Sharing for the Stream Control Transmission) s
5 262 M
(         Protocol \(SCTP\)", draft-tuexen-tsvwg-sctp-multipath-01 \(work in) s
5 251 M
(         progress\), December 2010.) s
5 229 M
(   [11]  Allman, M., "TCP Congestion Control with Appropriate Byte) s
5 218 M
(         Counting \(ABC\)", RFC 3465, February 2003.) s
5 196 M
(   [12]  Blanchet, M. and P. Seite, "Multiple Interfaces and) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 30]) s
_R
S
%%Page: (31) 31
%%BeginPageSetup
_S
18 36 translate
/pagenum 31 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(         Provisioning Domains Problem Statement",) s
5 702 M
(         draft-ietf-mif-problem-statement-09 \(work in progress\),) s
5 691 M
(         October 2010.) s
5 669 M
(   [13]  Draves, R., "Default Address Selection for Internet Protocol) s
5 658 M
(         version 6 \(IPv6\)", RFC 3484, February 2003.) s
5 636 M
(   [14]  Watson, R., "Protocol stacks and multicore scalability",) s
5 625 M
(         Presentation at Maastricht MPTCP workshop , Jul 2010, <http://) s
5 614 M
(         www.informatics.sussex.ac.uk/research/projects/ngn/slides/) s
5 603 M
(         msn10talks/watson-stack.pdf>.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 31]) s
_R
S
%%Page: (32) 32
%%BeginPageSetup
_S
18 36 translate
/pagenum 32 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(Appendix A.  Design alternatives) s
5 691 M
(   In this appendix, we describe alternate designs that have been) s
5 680 M
(   considered previously, and abandoned for various reasons \(detailed as) s
5 669 M
(   well\).  We keep them here for the archive and possible discussion.) s
5 658 M
(   We also describe some potential designs that have not been explored) s
5 647 M
(   yet but could reveal to be better in the future, in which case that) s
5 636 M
(   would be moved to the draft body.) s
5 614 M
(A.1.  Another way to consider Path Management) s
5 592 M
(   In a previous implementation of MPTCP, it was proposed that the) s
5 581 M
(   multipath transport had an even more abstract view of the paths in) s
5 570 M
(   use than what is described in Section 2.  In that design, the sub-) s
5 559 M
(   sockets all shared the same tuple \(saddr,sport,daddr,dport\), and was) s
5 548 M
(   disambiguated only by the path index.  The advantage is that the) s
5 537 M
(   Multipath Transport needs only to worry about how to efficiently) s
5 526 M
(   spread data among multiple paths, without any knowledge about the) s
5 515 M
(   addresses or ports used by each particular subflow.) s
5 493 M
(   That design was particularly well suited for using Shim6 as a Path) s
5 482 M
(   Manager, because Shim6 is already designed to work in the network) s
5 471 M
(   layer and rewrite addresses.  The first version of the Linux MPTCP) s
5 460 M
(   implementation was using Shim6 as path manager.  It looks also well) s
5 449 M
(   suited to path managers that don't use addresses \(e.g. path managers) s
5 438 M
(   that write a label in the packet header, later interpreted by the) s
5 427 M
(   network\).  Finally, it removes the need for the token in the) s
5 416 M
(   multipath transport \(connection identification is done naturally with) s
5 405 M
(   the tuple, shared by all subflows\).  The token hence becomes specific) s
5 394 M
(   to the built-in path manager, and can be just ignored with other path) s
5 383 M
(   managers \(the context tag plays a similar role in shim6, nothing is) s
5 372 M
(   needed if the path manager just sets labels to the packets\).) s
5 350 M
(   However, this cleaner separation between Multipath Transport and Path) s
5 339 M
(   Management suffers from three drawbacks:) s
5 317 M
(   o  It requires a heavy modification to the existing stacks, because) s
5 306 M
(      it modifies the current way to identify sockets in the stack.) s
5 295 M
(      They are currently unambiguously identified with the usual) s
5 284 M
(      5-tuple.  This architecture would require extending the 5-tuple) s
5 273 M
(      with the path index, given all subflows would share the same) s
5 262 M
(      5-tuple.) s
5 240 M
(   o  Although correctly implemented stacks could handle that new) s
5 229 M
(      endpoint identifier \(5-tuple+path index\), having several flows) s
5 218 M
(      with same 5-tuple could confuse middleboxes.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 32]) s
_R
S
%%Page: (33) 33
%%BeginPageSetup
_S
18 36 translate
/pagenum 33 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   o  When the path manager involves using several addresses, forcing) s
5 702 M
(      the same 5-tuple for all subflows at the Multipath Transport level) s
5 691 M
(      implies that the Path Manager needs to rewrite the address fields) s
5 680 M
(      of each packet.  That rewriting operation is simply avoided if the) s
5 669 M
(      sockets are bound to the addresses actually used to send the) s
5 658 M
(      packets.  Hence, this alternate design would involve avoidable) s
5 647 M
(      costs for path managers that belong to the "multi-address") s
5 636 M
(      category.) s
5 614 M
(A.2.  Implementing alternate Path Managers) s
5 592 M
(   In Section 2, the Path Manager is defined as an entity that maintains) s
5 581 M
(   a \(path_index<->endpoint_id\) mapping.  This is enough in the case of) s
5 570 M
(   the built-in path manager, because the segments are associated to a) s
5 559 M
(   path within the socket itself, thanks to its endpoint_id.  However,) s
5 548 M
(   it is expected that most other path managers will need to apply a) s
5 537 M
(   particular action, on a per-packet basis, to associate them with a) s
5 526 M
(   path.  Example actions could be writing a number in a field of the) s
5 515 M
(   segment or choosing a different gateway than the default one in the) s
5 504 M
(   routing table.  In an earlier version of Linux MPTCP, based on a) s
5 493 M
(   Shim6 Path Manager, the action was used and consisted in rewriting) s
5 482 M
(   the addresses of the packets.) s
5 460 M
(   To reflect the need for a per-packet action, the PM mapping table \(an) s
5 449 M
(   example of which is given in Table 1\) only needs to be extended with) s
5 438 M
(   an action field.  As an example of this, we show hereafter an example) s
5 427 M
(   mapping table for a Path Manager based on writing the path index into) s
5 416 M
(   a field of the packets.) s
5 394 M
(    +---------+------------+---------------+--------------------------+) s
5 383 M
(    |  token  | path index |  Endpoint id  | Action \(Write x in DSCP\) |) s
5 372 M
(    +---------+------------+---------------+--------------------------+) s
5 361 M
(    | token_1 |      1     | <A1,B1,0,pB1> |             1            |) s
5 350 M
(    |         |            |               |                          |) s
5 339 M
(    | token_1 |      2     | <A1,B1,0,pB1> |             2            |) s
5 328 M
(    |         |            |               |                          |) s
5 317 M
(    | token_1 |      3     | <A1,B1,0,pB1> |             3            |) s
5 306 M
(    |         |            |               |                          |) s
5 295 M
(    | token_1 |      4     | <A1,B1,0,pB1> |             4            |) s
5 284 M
(    |         |            |               |                          |) s
5 273 M
(    |         |            |               |                          |) s
5 262 M
(    | token_2 |      1     | <A1,B1,0,pB2> |             1            |) s
5 251 M
(    |         |            |               |                          |) s
5 240 M
(    | token_2 |      2     | <A1,B1,0,pB2> |             2            |) s
5 229 M
(    +---------+------------+---------------+--------------------------+) s
5 207 M
(            Table 4: Example mapping table for a label-based PM) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 33]) s
_R
S
%%Page: (34) 34
%%BeginPageSetup
_S
18 36 translate
/pagenum 34 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(A.3.  When to instantiate a new meta-socket ?) s
5 691 M
(   The meta-socket is responsible only for MPTCP-related operations.) s
5 680 M
(   This includes connection-level reordering for incoming data,) s
5 669 M
(   scheduling for outgoing data, and subflow management.  A natural) s
5 658 M
(   choice then would be to instantiate a new meta-socket only when the) s
5 647 M
(   peer has told us that it supports MPTCP.  In the server it is) s
5 636 M
(   naturally the case since the master subsocket is created upon the) s
5 625 M
(   reception of a SYN+MP_CAPABLE.  The client, however, instantiates its) s
5 614 M
(   master subsocket when the application issues a socket\(\) system call,) s
5 603 M
(   but needs to wait until the SYN+ACK to know whether its peer supports) s
5 592 M
(   MPTCP.  Yet, it must already provide its token in the SYN.) s
5 570 M
(   Linux MPTCP currently instantiates its client-side meta-socket when) s
5 559 M
(   the master-socket is created \(just like the server-side\).  The) s
5 548 M
(   drawback of this is that if after socket\(\), the application) s
5 537 M
(   subsequently issues a listen\(\), we have built a useless meta-socket.) s
5 526 M
(   The same happens if the peer SYN+ACK does not carry the MP_CAPABLE) s
5 515 M
(   option.  To avoid that, one may want to instantiate the meta-socket) s
5 504 M
(   upon reception of an MP_CAPABLE option.  But this implies that the) s
5 493 M
(   token \(sent in the SYN\), must be stored in some temporary place or in) s
5 482 M
(   the master subsocket until the meta-socket is built.) s
5 460 M
(A.4.  Forcing more processing in user context) s
5 438 M
(   The implementation architecture proposed in this draft uses the) s
5 427 M
(   following queue configuration:) s
5 405 M
(   o  Subflow level: out-of-order queue.  Used for subflow-level) s
5 394 M
(      reordering.) s
5 372 M
(   o  Connection level: out-of-order queue.  Used for connection-level) s
5 361 M
(      reordering.) s
5 339 M
(   o  Connection level: receive queue.  Used for storing the ordered) s
5 328 M
(      data until the application asks for it through a recvmsg\(\) system) s
5 317 M
(      call or similar.) s
5 295 M
(   In a previous version of Linux MPTCP, another queue configuration has) s
5 284 M
(   been examined:) s
5 262 M
(   o  Subflow level: out-of-order queue.  Used for subflow-level) s
5 251 M
(      reordering.) s
5 229 M
(   o  Subflow level: receive queue.  Used for storing the data until the) s
5 218 M
(      application asks for it through a recvmsg\(\) system call or) s
5 207 M
(      similar.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 34]) s
_R
S
%%Page: (35) 35
%%BeginPageSetup
_S
18 36 translate
/pagenum 35 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   o  Connection level: out-of-order queue.  Used for connection-level) s
5 702 M
(      reordering.) s
5 680 M
(   In this alternate architecture, the connection-level data is lazily) s
5 669 M
(   reordered as the application asks for it.  The main goal for this was) s
5 658 M
(   to ensure that as many CPU cycles as possible were spent in user) s
5 647 M
(   context \(See Section 3.1\).  VJ prequeues allow forcing user context) s
5 636 M
(   processing when the application is waiting on a recv\(\) system call.) s
5 625 M
(   Otherwise the subflow-level reordering must be done in interrupt) s
5 614 M
(   context.  This remains true with MPTCP because the subflow-level) s
5 603 M
(   implementation is left unmodified when possible.  With MPTCP, the) s
5 592 M
(   question is: "Where do we perform connection-level reordering ?".) s
5 581 M
(   This alternate architecture answer is: "Do it _always_ in user) s
5 570 M
(   context".  This was the strength of that architecture.  Technically,) s
5 559 M
(   the task of each subflow was to reorder its own segments and put them) s
5 548 M
(   in their own receive queue, until the application asks for data.) s
5 537 M
(   When the application wants to eat more data, MPTCP searches all) s
5 526 M
(   subflow-level receive queue for the next bytes to receive, and) s
5 515 M
(   reorder them as appropriate by using its own reordering queue.  As) s
5 504 M
(   soon as the number of requested bytes are handed to the application) s
5 493 M
(   buffer, the MPTCP reordering task finishes.) s
5 471 M
(   Unfortunately, there are two major drawbacks about doing it that way:) s
5 449 M
(   o  The socket API supports the SO_RCVLOWAT option, which allows an) s
5 438 M
(      application to ask not being woken up until n bytes have been) s
5 427 M
(      received.  Counting those bytes requires reordering at least n) s
5 416 M
(      bytes at the connection level in interrupt context.) s
5 394 M
(   o  The DATA_ACK [1] should report the latest byte received in order) s
5 383 M
(      at the connection level.  In this architecture, the best we can do) s
5 372 M
(      is report the latest byte that has been copied to the application) s
5 361 M
(      buffers, which would slightly change the DATA_ACK semantic) s
5 350 M
(      described in section 3.3.2 of [1].  This change could confuse) s
5 339 M
(      peers that try to derive information from the received DATA_ACK.) s
5 317 M
(A.5.  Buffering data on a per-subflow basis) s
5 295 M
(   In previous versions of Linux MPTCP, the configuration of the send) s
5 284 M
(   queues was as shown in Figure 4.) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 35]) s
_R
S
%%Page: (36) 36
%%BeginPageSetup
_S
18 36 translate
/pagenum 36 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(                                  Application) s
5 702 M
(                                      |) s
5 691 M
(                                      v) s
5 680 M
(                                Packet Scheduler) s
5 669 M
(                                     /  \\) s
5 658 M
(                                    /    \\) s
5 647 M
(                                   |      |) s
5 636 M
(                                   v      v) s
5 625 M
(                                 | * |  |   |) s
5 614 M
(    Next segment to send \(A\)  -> | * |  | * |) s
5 603 M
(                                 |---|  |---|  <- Separate send queue) s
5 592 M
(    Sent, but not acked \(B\)   -> |_*_|  |_*_|) s
5 581 M
(                                   |      |) s
5 570 M
(                                   v      v) s
5 559 M
(                                  NIC    NIC) s
5 537 M
(                    Figure 4: Send queue configuration) s
5 515 M
(   In contrast to the architecture presented in Section 3.5.2, there is) s
5 504 M
(   no shared send queue.  The Packet Scheduler is run each time data is) s
5 493 M
(   produced by the application.  Compared to Figure 4, the advantages) s
5 482 M
(   and drawbacks are basically reversed.  Here are the advantages:) s
5 460 M
(   o  This architecture supports subflow-specific Maximum Segment Sizes,) s
5 449 M
(      because the subflow is selected before the segment is built.) s
5 427 M
(   o  The segments are stored in their final form in the subflow-) s
5 416 M
(      specific send queues, and there is no need to run the Packet) s
5 405 M
(      Scheduler at transmission time.  The result is more fairness with) s
5 394 M
(      other applications \(because the Packet Scheduler runs in user) s
5 383 M
(      context only\), and faster data transmission when acknowledgements) s
5 372 M
(      open the congestion window \(because segments are buffered in their) s
5 361 M
(      final form and no call to the Packet Scheduler is needed.) s
5 339 M
(   The drawback, which motivated the architecture change in Linux MPTCP) s
5 328 M
(   is the complexity of the data allocation \(hence the Packet) s
5 317 M
(   Scheduler\), and the computing cost involved.  Given that there is no) s
5 306 M
(   shared send buffer, the send buffer auto-tuning must be divided into) s
5 295 M
(   its subflow contributions.  This buffer size can be easily derived) s
5 284 M
(   from Section 3.5.1.  However, when scheduling in advance a full send) s
5 273 M
(   buffer of data, we may be allocating a segment hundreds of) s
5 262 M
(   milliseconds before it actually goes to the wire.  The task of the) s
5 251 M
(   Packet Scheduler is then complicated because it must _predict_ the) s
5 240 M
(   path properties.  If the prediction is incorrect, two subflows may) s
5 229 M
(   try to put on the wire segments that are very distant in terms of) s
5 218 M
(   DATA_SEQ numbers.  This can eventually result in stalling some) s
5 207 M
(   subflows, because the DATA_SEQ gap between two subflows exceeds the) s
5 196 M
(   receive window announced by the receiver.  The Packet Scheduler can) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 36]) s
_R
S
%%Page: (37) 37
%%BeginPageSetup
_S
18 36 translate
/pagenum 37 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(   relatively easily compute a correct allocation of segments if the) s
5 702 M
(   path properties do not vary \(just because it is easy to predict a) s
5 691 M
(   constant value\), but the implementation was very sensitive to) s
5 680 M
(   variations in delay or bandwidth.  The previous implementation of) s
5 669 M
(   Linux MPTCP solved this allocation problem by verifying, upon each) s
5 658 M
(   failed transmission attempt, if it was blocked by the receive window) s
5 647 M
(   due to a gap in DATA_SEQ with other subflows.  If this was the case,) s
5 636 M
(   a full reallocation of segments was conducted.  However, the cost of) s
5 625 M
(   such a reallocation is very high, because it involves reconsidering) s
5 614 M
(   the allocation of any single segment, and do this for all the) s
5 603 M
(   subflows.  Worse, this costly reallocation sometimes needed to happen) s
5 592 M
(   in interrupt context, which removed one of the advantages of this) s
5 581 M
(   architecture.) s
5 559 M
(   Yet, under the assumption that the subflow-specific queue size is) s
5 548 M
(   small, the above drawback almost disappears.  For this reason the) s
5 537 M
(   abandoned design described here could be used to feed a future hybrid) s
5 526 M
(   architecture, as explained in Section 3.5.2.  For the sake of) s
5 515 M
(   comparison with Table 3, we provide hereafter the action/table) s
5 504 M
(   implemented by this architecture.) s
5 482 M
(   +-----------------+-------------------------------------------------+) s
5 471 M
(   | event           | action                                          |) s
5 460 M
(   +-----------------+-------------------------------------------------+) s
5 449 M
(   | Segment         | Remove references to it from the subflow-level  |) s
5 438 M
(   | acknowledged at | queue                                           |) s
5 427 M
(   | subflow level   |                                                 |) s
5 416 M
(   |                 |                                                 |) s
5 405 M
(   | Segment         | No queue-related action.                        |) s
5 394 M
(   | acknowledged at |                                                 |) s
5 383 M
(   | connection      |                                                 |) s
5 372 M
(   | level           |                                                 |) s
5 361 M
(   |                 |                                                 |) s
5 350 M
(   | Timeout         | Push the segment to the best subflow \(according |) s
5 339 M
(   | \(subflow-level\) | to the Packet Scheduler\). In contrast with the  |) s
5 328 M
(   |                 | solution of Section 3.5.2, there is no need for |) s
5 317 M
(   |                 | a connection-level retransmit queue, because    |) s
5 306 M
(   |                 | there is no requirement to be available         |) s
5 295 M
(   |                 | immediately for a subflow to accept new data.   |) s
5 284 M
(   |                 |                                                 |) s
5 273 M
(   | Ready to put    | Just send the next segment from the A portion   |) s
5 262 M
(   | new data on the | of the subflow-specific send queue, if any.     |) s
5 251 M
(   | wire \(normally  | Note that the "IMPORTANT" note from             |) s
5 240 M
(   | triggered by an | Section 3.5.2 still applies with this           |) s
5 229 M
(   | incoming ack\)   | architecture.                                   |) s
5 218 M
(   +-----------------+-------------------------------------------------+) s
5 196 M
(   Table 5: \(event,action\) pairs implemented in a queue management based) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 37]) s
_R
S
%%Page: (38) 38
%%BeginPageSetup
_S
18 36 translate
/pagenum 38 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(                          on separate send queues) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 38]) s
_R
S
%%Page: (39) 39
%%BeginPageSetup
_S
18 36 translate
/pagenum 39 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(Appendix B.  Ongoing discussions on implementation improvements) s
5 691 M
(   This appendix collects information on features that have been) s
5 680 M
(   currently implemented nowhere, but can still be useful as hints for) s
5 669 M
(   implementers to test.  Feedback from implementers will help) s
5 658 M
(   converging on those topics and propose solid guidelines for future) s
5 647 M
(   versions of this memo.) s
5 625 M
(B.1.  Heuristics for subflow management) s
5 603 M
(   Some heuristic should determine when it would be beneficial to add a) s
5 592 M
(   new subflow.  Linux MPTCP has no such heuristic at the moment, but) s
5 581 M
(   the topic has been discussed on the MPTCP mailing list, so this) s
5 570 M
(   section summarizes the input from many individuals.  MPTCP is not) s
5 559 M
(   useful for very short flows, so three questions appear:) s
5 537 M
(   o  How long is a "too short flow") s
5 515 M
(   o  How to predict that a flow will be short ?) s
5 493 M
(   o  When to decide to add/remove subflows ?) s
5 471 M
(   To answer the third question, it has been proposed to use hints from) s
5 460 M
(   the application.  On the other hand the experience shows that socket) s
5 449 M
(   options are quite often poorly or not used, which motivates the) s
5 438 M
(   parallel use of a good default heuristic.  This default heuristic may) s
5 427 M
(   be influenced in particular by the particular set of options that are) s
5 416 M
(   enabled for MPTCP \(e.g. an administrator can decide that some) s
5 405 M
(   security mechanisms for subflow initiation are not needed in his) s
5 394 M
(   environment, and disable them, which would change the cost of) s
5 383 M
(   establishing new subflows\).  The following elements have been) s
5 372 M
(   proposed to feed the heuristic, none of them tested yet:) s
5 350 M
(   o  Check the size of the write operations from the applications.) s
5 339 M
(      Initiate a new subflow if the write size exceeds some threshold.) s
5 328 M
(      This information can be taken only as a hint because applications) s
5 317 M
(      could send big chunks of data split in many small writes.  A) s
5 306 M
(      particular case of checking the size of write operations is when) s
5 295 M
(      the application uses the sendfile\(\) system call.  In that) s
5 284 M
(      situation MPTCP can know very precisely how many bytes will be) s
5 273 M
(      transferred.) s
5 251 M
(   o  Check if the flow is network limited or application limited.) s
5 240 M
(      Initiate a new subflow only if it is network limited.) s
5 218 M
(   o  It may be useful to establish new subflows even for application-) s
5 207 M
(      limited communications, to provide failure survivability.  A way) s
5 196 M
(      to do that would be to initiate a new subflow \(if not done before) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 39]) s
_R
S
%%Page: (40) 40
%%BeginPageSetup
_S
18 36 translate
/pagenum 40 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(      by another trigger\) after some time has elapsed, regardless of) s
5 702 M
(      whether the communication is network or application limited.) s
5 680 M
(   o  Wait until slow start is done before to establish a new subflow.) s
5 669 M
(      Measurements with Linux MPTCP suggest that slow start could be a) s
5 658 M
(      reasonable tool for determining when it is worth starting a new) s
5 647 M
(      subflow \(without increasing the overall completion time\).  More) s
5 636 M
(      analysis is needed in that area, however.  Also, this should be) s
5 625 M
(      taken as a hint only if the slow start is actually progressing) s
5 614 M
(      \(otherwise a stalled subflow could prevent the establishment of) s
5 603 M
(      another one, precisely when a new one would be useful\).) s
5 581 M
(   o  Use information from the application-layer protocol.  Some of them) s
5 570 M
(      \(e.g.  HTTP\) carry flow length information in their headers, which) s
5 559 M
(      can be used to decide how many subflows are useful.) s
5 537 M
(   o  Allow the administrator to configure subflow policies on a per-) s
5 526 M
(      port basis.  The host stack could learn as well for what ports) s
5 515 M
(      MPTCP turns out to be useful.) s
5 493 M
(   o  Check the underlying medium of each potential subflow.  For) s
5 482 M
(      example, if the initial subflow is initiated over 3G, and WiFi is) s
5 471 M
(      available, it probably makes sense to immediately negotiate an) s
5 460 M
(      additional subflow over WiFi.) s
5 438 M
(   It is not only useful to determine when to start new subflows, one) s
5 427 M
(   should also sometimes decide to abandon some of its subflows.  An) s
5 416 M
(   MPTCP implementation should be able to determine when removing a) s
5 405 M
(   subflow would increase the aggregate bandwidth.  This can happen, for) s
5 394 M
(   example, when the subflow has a significantly higher delay compared) s
5 383 M
(   to other subflows, and the maximum buffer size allowed by the) s
5 372 M
(   administrator has been reached \(Linux MPTCP currently has no such) s
5 361 M
(   heuristic yet\).) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 40]) s
_R
S
%%Page: (41) 41
%%BeginPageSetup
_S
18 36 translate
/pagenum 41 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
5 746 M
(Internet-Draft           MPTCP Impl. guidelines               March 2011) s
5 713 M
(Authors' Addresses) s
5 691 M
(   Sebastien Barre) s
5 680 M
(   Universite catholique de Louvain) s
5 669 M
(   Place Ste Barbe, 2) s
5 658 M
(   Louvain-la-Neuve  1348) s
5 647 M
(   BE) s
5 625 M
(   Email: sebastien.barre@uclouvain.be) s
5 614 M
(   URI:   http://inl.info.ucl.ac.be/sbarre) s
5 581 M
(   Christoph Paasch) s
5 570 M
(   Universite catholique de Louvain) s
5 559 M
(   Place Ste Barbe, 2) s
5 548 M
(   Louvain-la-Neuve  1348) s
5 537 M
(   BE) s
5 515 M
(   Email: christoph.paasch@uclouvain.be) s
5 504 M
(   URI:   http://inl.info.ucl.ac.be/cpaasch) s
5 471 M
(   Olivier Bonaventure) s
5 460 M
(   Universite catholique de Louvain) s
5 449 M
(   Place Ste Barbe, 2) s
5 438 M
(   Louvain-la-Neuve  1348) s
5 427 M
(   BE) s
5 405 M
(   URI:   http://inl.info.ucl.ac.be/obo) s
5 152 M
(Barre, et al.           Expires September 8, 2011              [Page 41]) s
_R
S
%%Page: (42) 42
%%BeginPageSetup
_S
18 36 translate
/pagenum 42 def
/fname (draft-barre-mptcp-impl-00.txt) def
/fdir (.) def
/ftail (draft-barre-mptcp-impl-00.txt) def
/user_header_p false def
/user_footer_p false def
%%EndPageSetup
_R
S
%%Trailer
%%Pages: 42
%%DocumentNeededResources: font Courier-Bold Courier 
%%EOF
